{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04163a7c-056b-4b84-a756-96da8a702460",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "\n",
    "Re-install YOLOv7 package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52256233-317d-430a-b70b-ecae4b8489fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip uninstall -y yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca6f1ef-3b8d-4261-b574-2293c60dac2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U git+https://github.com/ValV/yolov7.git@package#egg=yolov7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77604a1d-bdd2-4fe8-86f2-09ffe283ccfc",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "This training notebook is based on three elephants:\n",
    "* `train` - this is the main function to train a YOLOv7 model;\n",
    "* `test` - this function is used by `train` on validation step;\n",
    "* `export` - this function is auxiliary and is defined at the end.\n",
    "\n",
    "Imports for `train` and `test` functions necessary for the train loop are separated from imports for the `export` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8222dc6e-8a31-4b2f-be38-fdc2a1bdb375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "\n",
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.utils.data\n",
    "import yaml\n",
    "from torch.cuda import amp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import test  # FIXME: import test.py to get mAP after each epoch\n",
    "\n",
    "from yolov7 import PACKAGE_ROOT\n",
    "from yolov7.models.experimental import attempt_load\n",
    "from yolov7.models.yolo import Model\n",
    "from yolov7.utils.autoanchor import check_anchors\n",
    "from yolov7.utils.datasets import create_dataloader\n",
    "from yolov7.utils.general import (\n",
    "    labels_to_class_weights,\n",
    "    increment_path,\n",
    "    labels_to_image_weights,\n",
    "    init_seeds,\n",
    "    strip_optimizer,\n",
    "    get_latest_run,\n",
    "    check_dataset,\n",
    "    check_file,\n",
    "    check_git_status,\n",
    "    check_img_size,\n",
    "    check_requirements,\n",
    "    print_mutation,\n",
    "    set_logging,\n",
    "    one_cycle,\n",
    "    colorstr,\n",
    ")\n",
    "from yolov7.utils.general import (\n",
    "    coco80_to_coco91_class,\n",
    "    check_dataset,\n",
    "    check_file,\n",
    "    check_img_size,\n",
    "    check_requirements,\n",
    "    box_iou,\n",
    "    non_max_suppression,\n",
    "    scale_coords,\n",
    "    xyxy2xywh,\n",
    "    xywh2xyxy,\n",
    "    set_logging,\n",
    "    increment_path,\n",
    "    colorstr,\n",
    ")  # Test\n",
    "from yolov7.utils.google_utils import attempt_download\n",
    "from yolov7.utils.loss import ComputeLoss, ComputeLossOTA\n",
    "from yolov7.utils.metrics import fitness\n",
    "from yolov7.utils.metrics import ap_per_class, ConfusionMatrix  # Test\n",
    "from yolov7.utils.plots import (\n",
    "    plot_images,\n",
    "    plot_labels,\n",
    "    plot_results,\n",
    "    plot_evolution,\n",
    ")\n",
    "from yolov7.utils.plots import (\n",
    "    plot_images,\n",
    "    output_to_target,\n",
    "    plot_study_txt,\n",
    ")  # Test\n",
    "from yolov7.utils.torch_utils import (\n",
    "    ModelEMA,\n",
    "    select_device,\n",
    "    intersect_dicts,\n",
    "    torch_distributed_zero_first,\n",
    "    is_parallel,\n",
    ")\n",
    "from yolov7.utils.torch_utils import (\n",
    "    select_device,\n",
    "    time_synchronized,\n",
    "    TracedModel,\n",
    ")  # Test\n",
    "from yolov7.utils.wandb_logging.wandb_utils import (\n",
    "    WandbLogger,\n",
    "    check_wandb_resume,\n",
    ")\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f6e09f-1634-4a59-b68c-bf0af304c717",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "The `test` function must be defined before `train`, since the latter depends on it.\n",
    "\n",
    "This function is taken from [test.py](https://github.com/ValV/yolov7/blob/master/test.py) so that it would be easy to modify it in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea08d397-074f-47fe-94e5-adeb2543a411",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(\n",
    "    data,\n",
    "    weights=None,\n",
    "    batch_size=32,\n",
    "    imgsz=640,\n",
    "    conf_thres=0.001,\n",
    "    iou_thres=0.45,  # for NMS (default: 0.6)\n",
    "    save_json=False,\n",
    "    single_cls=False,\n",
    "    augment=False,\n",
    "    verbose=False,\n",
    "    model=None,\n",
    "    dataloader=None,\n",
    "    save_dir=Path(\"\"),  # for saving images\n",
    "    save_txt=False,  # for auto-labelling\n",
    "    save_hybrid=False,  # for hybrid auto-labelling\n",
    "    save_conf=False,  # save auto-label confidences\n",
    "    plots=True,\n",
    "    wandb_logger=None,\n",
    "    compute_loss=None,\n",
    "    half_precision=True,\n",
    "    trace=False,\n",
    "    is_coco=False,\n",
    "    v5_metric=False,\n",
    "):\n",
    "    # Initialize/load model and set device\n",
    "    training = model is not None\n",
    "    if training:  # called by train.py\n",
    "        device = next(model.parameters()).device  # get model device\n",
    "\n",
    "    else:  # called directly\n",
    "        set_logging()\n",
    "        device = select_device(opt.device, batch_size=batch_size)\n",
    "\n",
    "        # Directories\n",
    "        save_dir = Path(\n",
    "            increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok)\n",
    "        )  # increment run\n",
    "        (save_dir / \"labels\" if save_txt else save_dir).mkdir(\n",
    "            parents=True, exist_ok=True\n",
    "        )  # make dir\n",
    "\n",
    "        # Load model\n",
    "        model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "        gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
    "        imgsz = check_img_size(imgsz, s=gs)  # check img_size\n",
    "\n",
    "        if trace:\n",
    "            model = TracedModel(model, device, imgsz)\n",
    "\n",
    "    # Half\n",
    "    half = (\n",
    "        device.type != \"cpu\" and half_precision\n",
    "    )  # half precision only supported on CUDA\n",
    "    if half:\n",
    "        model.half()\n",
    "\n",
    "    # Configure\n",
    "    model.eval()\n",
    "    if isinstance(data, str):\n",
    "        is_coco = data.endswith(\"coco.yaml\")\n",
    "        with open(data) as f:\n",
    "            data = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "    check_dataset(data)  # check\n",
    "    nc = 1 if single_cls else int(data[\"nc\"])  # number of classes\n",
    "    iouv = torch.linspace(0.5, 0.95, 10).to(device)  # iou vector for mAP@0.5:0.95\n",
    "    niou = iouv.numel()\n",
    "\n",
    "    # Logging\n",
    "    log_imgs = 0\n",
    "    if wandb_logger and wandb_logger.wandb:\n",
    "        log_imgs = min(wandb_logger.log_imgs, 100)\n",
    "    # Dataloader\n",
    "    if not training:\n",
    "        if device.type != \"cpu\":\n",
    "            model(\n",
    "                torch.zeros(1, 3, imgsz, imgsz)\n",
    "                .to(device)\n",
    "                .type_as(next(model.parameters()))\n",
    "            )  # run once\n",
    "        task = (\n",
    "            opt.task if opt.task in (\"train\", \"val\", \"test\") else \"val\"\n",
    "        )  # path to train/val/test images\n",
    "        dataloader = create_dataloader(\n",
    "            data[task],\n",
    "            imgsz,\n",
    "            batch_size,\n",
    "            gs,\n",
    "            opt,\n",
    "            pad=0.5,\n",
    "            rect=True,\n",
    "            prefix=colorstr(f\"{task}: \"),\n",
    "        )[0]\n",
    "\n",
    "    if v5_metric:\n",
    "        print(\"Testing with YOLOv5 AP metric...\")\n",
    "\n",
    "    seen = 0\n",
    "    confusion_matrix = ConfusionMatrix(nc=nc)\n",
    "    names = {\n",
    "        k: v\n",
    "        for k, v in enumerate(\n",
    "            model.names if hasattr(model, \"names\") else model.module.names\n",
    "        )\n",
    "    }\n",
    "    coco91class = coco80_to_coco91_class()\n",
    "    s = (\"%20s\" + \"%12s\" * 6) % (\n",
    "        \"Class\",\n",
    "        \"Images\",\n",
    "        \"Labels\",\n",
    "        \"P\",\n",
    "        \"R\",\n",
    "        \"mAP@.5\",\n",
    "        \"mAP@.5:.95\",\n",
    "    )\n",
    "    p, r, f1, mp, mr, map50, map, t0, t1 = (\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "    )\n",
    "    loss = torch.zeros(3, device=device)\n",
    "    jdict, stats, ap, ap_class, wandb_images = [], [], [], [], []\n",
    "    for batch_i, (img, targets, paths, shapes) in enumerate(tqdm(dataloader, desc=s)):\n",
    "        img = img.to(device, non_blocking=True)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        targets = targets.to(device)\n",
    "        nb, _, height, width = img.shape  # batch size, channels, height, width\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Run model\n",
    "            t = time_synchronized()\n",
    "            out, train_out = model(\n",
    "                img, augment=augment\n",
    "            )  # inference and training outputs\n",
    "            t0 += time_synchronized() - t\n",
    "\n",
    "            # Compute loss\n",
    "            if compute_loss:\n",
    "                loss += compute_loss([x.float() for x in train_out], targets)[1][\n",
    "                    :3\n",
    "                ]  # box, obj, cls\n",
    "\n",
    "            # Run NMS\n",
    "            targets[:, 2:] *= torch.Tensor([width, height, width, height]).to(\n",
    "                device\n",
    "            )  # to pixels\n",
    "            lb = (\n",
    "                [targets[targets[:, 0] == i, 1:] for i in range(nb)]\n",
    "                if save_hybrid\n",
    "                else []\n",
    "            )  # for autolabelling\n",
    "            t = time_synchronized()\n",
    "            out = non_max_suppression(\n",
    "                out,\n",
    "                conf_thres=conf_thres,\n",
    "                iou_thres=iou_thres,\n",
    "                labels=lb,\n",
    "                multi_label=True,\n",
    "            )\n",
    "            t1 += time_synchronized() - t\n",
    "\n",
    "        # Statistics per image\n",
    "        for si, pred in enumerate(out):\n",
    "            labels = targets[targets[:, 0] == si, 1:]\n",
    "            nl = len(labels)\n",
    "            tcls = labels[:, 0].tolist() if nl else []  # target class\n",
    "            path = Path(paths[si])\n",
    "            seen += 1\n",
    "\n",
    "            if len(pred) == 0:\n",
    "                if nl:\n",
    "                    stats.append(\n",
    "                        (\n",
    "                            torch.zeros(0, niou, dtype=torch.bool),\n",
    "                            torch.Tensor(),\n",
    "                            torch.Tensor(),\n",
    "                            tcls,\n",
    "                        )\n",
    "                    )\n",
    "                continue\n",
    "\n",
    "            # Predictions\n",
    "            predn = pred.clone()\n",
    "            scale_coords(\n",
    "                img[si].shape[1:], predn[:, :4], shapes[si][0], shapes[si][1]\n",
    "            )  # native-space pred\n",
    "\n",
    "            # Append to text file\n",
    "            if save_txt:\n",
    "                gn = torch.tensor(shapes[si][0])[\n",
    "                    [1, 0, 1, 0]\n",
    "                ]  # normalization gain whwh\n",
    "                for *xyxy, conf, cls in predn.tolist():\n",
    "                    xywh = (\n",
    "                        (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn)\n",
    "                        .view(-1)\n",
    "                        .tolist()\n",
    "                    )  # normalized xywh\n",
    "                    line = (\n",
    "                        (cls, *xywh, conf) if save_conf else (cls, *xywh)\n",
    "                    )  # label format\n",
    "                    with open(save_dir / \"labels\" / (path.stem + \".txt\"), \"a\") as f:\n",
    "                        f.write((\"%g \" * len(line)).rstrip() % line + \"\\n\")\n",
    "\n",
    "            # W&B logging - Media Panel Plots\n",
    "            if (\n",
    "                len(wandb_images) < log_imgs and wandb_logger.current_epoch > 0\n",
    "            ):  # Check for test operation\n",
    "                if wandb_logger.current_epoch % wandb_logger.bbox_interval == 0:\n",
    "                    box_data = [\n",
    "                        {\n",
    "                            \"position\": {\n",
    "                                \"minX\": xyxy[0],\n",
    "                                \"minY\": xyxy[1],\n",
    "                                \"maxX\": xyxy[2],\n",
    "                                \"maxY\": xyxy[3],\n",
    "                            },\n",
    "                            \"class_id\": int(cls),\n",
    "                            \"box_caption\": \"%s %.3f\" % (names[cls], conf),\n",
    "                            \"scores\": {\"class_score\": conf},\n",
    "                            \"domain\": \"pixel\",\n",
    "                        }\n",
    "                        for *xyxy, conf, cls in pred.tolist()\n",
    "                    ]\n",
    "                    boxes = {\n",
    "                        \"predictions\": {\n",
    "                            \"box_data\": box_data,\n",
    "                            \"class_labels\": names,\n",
    "                        }\n",
    "                    }  # inference-space\n",
    "                    wandb_images.append(\n",
    "                        wandb_logger.wandb.Image(\n",
    "                            img[si], boxes=boxes, caption=path.name\n",
    "                        )\n",
    "                    )\n",
    "            (\n",
    "                wandb_logger.log_training_progress(predn, path, names)\n",
    "                if wandb_logger and wandb_logger.wandb_run\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            # Append to pycocotools JSON dictionary\n",
    "            if save_json:\n",
    "                # [{\"image_id\": 42, \"category_id\": 18, \"bbox\": [258.15, 41.29, 348.26, 243.78], \"score\": 0.236}, ...\n",
    "                image_id = int(path.stem) if path.stem.isnumeric() else path.stem\n",
    "                box = xyxy2xywh(predn[:, :4])  # xywh\n",
    "                box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\n",
    "                for p, b in zip(pred.tolist(), box.tolist()):\n",
    "                    jdict.append(\n",
    "                        {\n",
    "                            \"image_id\": image_id,\n",
    "                            \"category_id\": (\n",
    "                                coco91class[int(p[5])] if is_coco else int(p[5])\n",
    "                            ),\n",
    "                            \"bbox\": [round(x, 3) for x in b],\n",
    "                            \"score\": round(p[4], 5),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            # Assign all predictions as incorrect\n",
    "            correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool, device=device)\n",
    "            if nl:\n",
    "                detected = []  # target indices\n",
    "                tcls_tensor = labels[:, 0]\n",
    "\n",
    "                # target boxes\n",
    "                tbox = xywh2xyxy(labels[:, 1:5])\n",
    "                scale_coords(\n",
    "                    img[si].shape[1:], tbox, shapes[si][0], shapes[si][1]\n",
    "                )  # native-space labels\n",
    "                if plots:\n",
    "                    confusion_matrix.process_batch(\n",
    "                        predn, torch.cat((labels[:, 0:1], tbox), 1)\n",
    "                    )\n",
    "\n",
    "                # Per target class\n",
    "                for cls in torch.unique(tcls_tensor):\n",
    "                    ti = (\n",
    "                        (cls == tcls_tensor).nonzero(as_tuple=False).view(-1)\n",
    "                    )  # prediction indices\n",
    "                    pi = (\n",
    "                        (cls == pred[:, 5]).nonzero(as_tuple=False).view(-1)\n",
    "                    )  # target indices\n",
    "\n",
    "                    # Search for detections\n",
    "                    if pi.shape[0]:\n",
    "                        # Prediction to target ious\n",
    "                        ious, i = box_iou(predn[pi, :4], tbox[ti]).max(\n",
    "                            1\n",
    "                        )  # best ious, indices\n",
    "\n",
    "                        # Append detections\n",
    "                        detected_set = set()\n",
    "                        for j in (ious > iouv[0]).nonzero(as_tuple=False):\n",
    "                            d = ti[i[j]]  # detected target\n",
    "                            if d.item() not in detected_set:\n",
    "                                detected_set.add(d.item())\n",
    "                                detected.append(d)\n",
    "                                correct[pi[j]] = ious[j] > iouv  # iou_thres is 1xn\n",
    "                                if (\n",
    "                                    len(detected) == nl\n",
    "                                ):  # all targets already located in image\n",
    "                                    break\n",
    "\n",
    "            # Append statistics (correct, conf, pcls, tcls)\n",
    "            stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))\n",
    "\n",
    "        # Plot images\n",
    "        if plots and batch_i < 3:\n",
    "            f = save_dir / f\"test_batch{batch_i}_labels.jpg\"  # labels\n",
    "            Thread(\n",
    "                target=plot_images,\n",
    "                args=(img, targets, paths, f, names),\n",
    "                daemon=True,\n",
    "            ).start()\n",
    "            f = save_dir / f\"test_batch{batch_i}_pred.jpg\"  # predictions\n",
    "            Thread(\n",
    "                target=plot_images,\n",
    "                args=(img, output_to_target(out), paths, f, names),\n",
    "                daemon=True,\n",
    "            ).start()\n",
    "\n",
    "    # Compute statistics\n",
    "    stats = [np.concatenate(x, 0) for x in zip(*stats)]  # to numpy\n",
    "    if len(stats) and stats[0].any():\n",
    "        p, r, ap, f1, ap_class = ap_per_class(\n",
    "            *stats,\n",
    "            plot=plots,\n",
    "            v5_metric=v5_metric,\n",
    "            save_dir=save_dir,\n",
    "            names=names,\n",
    "        )\n",
    "        ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
    "        mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
    "        nt = np.bincount(\n",
    "            stats[3].astype(np.int64), minlength=nc\n",
    "        )  # number of targets per class\n",
    "    else:\n",
    "        nt = torch.zeros(1)\n",
    "\n",
    "    # Print results\n",
    "    pf = \"%20s\" + \"%12i\" * 2 + \"%12.3g\" * 4  # print format\n",
    "    print(pf % (\"all\", seen, nt.sum(), mp, mr, map50, map))\n",
    "\n",
    "    # Print results per class\n",
    "    if (verbose or (nc < 50 and not training)) and nc > 1 and len(stats):\n",
    "        for i, c in enumerate(ap_class):\n",
    "            print(pf % (names[c], seen, nt[c], p[i], r[i], ap50[i], ap[i]))\n",
    "\n",
    "    # Print speeds\n",
    "    t = tuple(x / seen * 1e3 for x in (t0, t1, t0 + t1)) + (\n",
    "        imgsz,\n",
    "        imgsz,\n",
    "        batch_size,\n",
    "    )  # tuple\n",
    "    if not training:\n",
    "        print(\n",
    "            \"Speed: %.1f/%.1f/%.1f ms inference/NMS/total per %gx%g image at batch-size %g\"\n",
    "            % t\n",
    "        )\n",
    "\n",
    "    # Plots\n",
    "    if plots:\n",
    "        confusion_matrix.plot(save_dir=save_dir, names=list(names.values()))\n",
    "        if wandb_logger and wandb_logger.wandb:\n",
    "            val_batches = [\n",
    "                wandb_logger.wandb.Image(str(f), caption=f.name)\n",
    "                for f in sorted(save_dir.glob(\"test*.jpg\"))\n",
    "            ]\n",
    "            wandb_logger.log({\"Validation\": val_batches})\n",
    "    if wandb_images:\n",
    "        wandb_logger.log({\"Bounding Box Debugger/Images\": wandb_images})\n",
    "\n",
    "    # Save JSON\n",
    "    if save_json and len(jdict):\n",
    "        w = (\n",
    "            Path(weights[0] if isinstance(weights, list) else weights).stem\n",
    "            if weights is not None\n",
    "            else \"\"\n",
    "        )  # weights\n",
    "        anno_json = \"./coco/annotations/instances_val2017.json\"  # annotations json\n",
    "        pred_json = str(save_dir / f\"{w}_predictions.json\")  # predictions json\n",
    "        print(\"\\nEvaluating pycocotools mAP... saving %s...\" % pred_json)\n",
    "        with open(pred_json, \"w\") as f:\n",
    "            json.dump(jdict, f)\n",
    "\n",
    "        try:  # https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb\n",
    "            from pycocotools.coco import COCO\n",
    "            from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "            anno = COCO(anno_json)  # init annotations api\n",
    "            pred = anno.loadRes(pred_json)  # init predictions api\n",
    "            eval = COCOeval(anno, pred, \"bbox\")\n",
    "            if is_coco:\n",
    "                eval.params.imgIds = [\n",
    "                    int(Path(x).stem) for x in dataloader.dataset.img_files\n",
    "                ]  # image IDs to evaluate\n",
    "            eval.evaluate()\n",
    "            eval.accumulate()\n",
    "            eval.summarize()\n",
    "            map, map50 = eval.stats[:2]  # update results (mAP@0.5:0.95, mAP@0.5)\n",
    "        except Exception as e:\n",
    "            print(f\"pycocotools unable to run: {e}\")\n",
    "\n",
    "    # Return results\n",
    "    model.float()  # for training\n",
    "    if not training:\n",
    "        s = (\n",
    "            f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\"\n",
    "            if save_txt\n",
    "            else \"\"\n",
    "        )\n",
    "        print(f\"Results saved to {save_dir}{s}\")\n",
    "    maps = np.zeros(nc) + map\n",
    "    for i, c in enumerate(ap_class):\n",
    "        maps[c] = ap[i]\n",
    "    return (\n",
    "        (mp, mr, map50, map, *(loss.cpu() / len(dataloader)).tolist()),\n",
    "        maps,\n",
    "        t,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8ffc1-e13c-4723-8ab7-df1df622e750",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Main training entry-point, uses `test` function for evaluation (as mentioned above).\n",
    "\n",
    "This `train` function is taken from [train.py](https://github.com/ValV/yolov7/blob/master/train.py) for the same purpose as the `test` function (ease of modification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9011026c-7692-46b1-be05-b10942373ba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(hyp, opt, device, tb_writer=None):\n",
    "    if PACKAGE_ROOT in sys.path:\n",
    "        del sys.path[sys.path.index(PACKAGE_ROOT)]\n",
    "    if PACKAGE_ROOT not in sys.path:\n",
    "        # print(f\"DEBUG: adding '{PACKAGE_ROOT}' to sys.path...\")\n",
    "        sys.path.insert(0, PACKAGE_ROOT)  # FIXME: add models to path for pretrain\n",
    "    logger.info(\n",
    "        colorstr(\"hyperparameters: \") + \", \".join(f\"{k}={v}\" for k, v in hyp.items())\n",
    "    )\n",
    "    save_dir, epochs, batch_size, total_batch_size, weights, rank, freeze = (\n",
    "        Path(opt.save_dir),\n",
    "        opt.epochs,\n",
    "        opt.batch_size,\n",
    "        opt.total_batch_size,\n",
    "        opt.weights,\n",
    "        opt.global_rank,\n",
    "        opt.freeze,\n",
    "    )\n",
    "\n",
    "    # Directories\n",
    "    wdir = save_dir / \"weights\"\n",
    "    wdir.mkdir(parents=True, exist_ok=True)  # make dir\n",
    "    last = wdir / \"last.pt\"\n",
    "    best = wdir / \"best.pt\"\n",
    "    results_file = save_dir / \"results.txt\"\n",
    "\n",
    "    # Save run settings\n",
    "    with open(save_dir / \"hyp.yaml\", \"w\") as f:\n",
    "        yaml.dump(hyp, f, sort_keys=False)\n",
    "    with open(save_dir / \"opt.yaml\", \"w\") as f:\n",
    "        yaml.dump(vars(opt), f, sort_keys=False)\n",
    "\n",
    "    # Configure\n",
    "    plots = not opt.evolve  # create plots\n",
    "    cuda = device.type != \"cpu\"\n",
    "    init_seeds(2 + rank)\n",
    "    with open(opt.data) as f:\n",
    "        data_dict = yaml.load(f, Loader=yaml.SafeLoader)  # data dict\n",
    "    is_coco = opt.data.endswith(\"coco.yaml\")\n",
    "\n",
    "    # Logging - doing this before checking the dataset. Might update data_dict\n",
    "    loggers = {\"wandb\": None}  # loggers dict\n",
    "    if rank in [-1, 0]:\n",
    "        opt.hyp = hyp  # add hyperparameters\n",
    "        run_id = (\n",
    "            torch.load(weights, map_location=device).get(\"wandb_id\")\n",
    "            if weights.endswith(\".pt\") and os.path.isfile(weights)\n",
    "            else None\n",
    "        )\n",
    "        wandb_logger = WandbLogger(opt, Path(opt.save_dir).stem, run_id, data_dict)\n",
    "        loggers[\"wandb\"] = wandb_logger.wandb\n",
    "        data_dict = wandb_logger.data_dict\n",
    "        if wandb_logger.wandb:\n",
    "            weights, epochs, hyp = (\n",
    "                opt.weights,\n",
    "                opt.epochs,\n",
    "                opt.hyp,\n",
    "            )  # WandbLogger might update weights, epochs if resuming\n",
    "\n",
    "    nc = 1 if opt.single_cls else int(data_dict[\"nc\"])  # number of classes\n",
    "    names = (\n",
    "        [\"item\"]\n",
    "        if opt.single_cls and len(data_dict[\"names\"]) != 1\n",
    "        else data_dict[\"names\"]\n",
    "    )  # class names\n",
    "    assert len(names) == nc, \"%g names found for nc=%g dataset in %s\" % (\n",
    "        len(names),\n",
    "        nc,\n",
    "        opt.data,\n",
    "    )  # check\n",
    "\n",
    "    # Model\n",
    "    pretrained = weights.endswith(\".pt\")\n",
    "    # print(f\"DEBUG: weights = {weights}\")\n",
    "    if pretrained:\n",
    "        with torch_distributed_zero_first(rank):\n",
    "            # print(f\"DEBUG: trying to download '{weights}'\")\n",
    "            attempt_download(weights)  # download if not found locally\n",
    "        ckpt = torch.load(weights, map_location=device)  # load checkpoint\n",
    "        model = Model(\n",
    "            opt.cfg or ckpt[\"model\"].yaml,\n",
    "            ch=3,\n",
    "            nc=nc,\n",
    "            anchors=hyp.get(\"anchors\"),\n",
    "        ).to(device)  # create\n",
    "        exclude = (\n",
    "            [\"anchor\"] if (opt.cfg or hyp.get(\"anchors\")) and not opt.resume else []\n",
    "        )  # exclude keys\n",
    "        state_dict = ckpt[\"model\"].float().state_dict()  # to FP32\n",
    "        state_dict = intersect_dicts(\n",
    "            state_dict, model.state_dict(), exclude=exclude\n",
    "        )  # intersect\n",
    "        model.load_state_dict(state_dict, strict=False)  # load\n",
    "        logger.info(\n",
    "            \"Transferred %g/%g items from %s\"\n",
    "            % (len(state_dict), len(model.state_dict()), weights)\n",
    "        )  # report\n",
    "    else:\n",
    "        model = Model(opt.cfg, ch=3, nc=nc, anchors=hyp.get(\"anchors\")).to(\n",
    "            device\n",
    "        )  # create\n",
    "    with torch_distributed_zero_first(rank):\n",
    "        check_dataset(data_dict)  # check\n",
    "    train_path = data_dict[\"train\"]\n",
    "    test_path = data_dict[\"val\"]\n",
    "\n",
    "    # Freeze\n",
    "    freeze = [\n",
    "        f\"model.{x}.\" for x in (freeze if len(freeze) > 1 else range(freeze[0]))\n",
    "    ]  # parameter names to freeze (full or partial)\n",
    "    for k, v in model.named_parameters():\n",
    "        v.requires_grad = True  # train all layers\n",
    "        if any(x in k for x in freeze):\n",
    "            print(\"freezing %s\" % k)\n",
    "            v.requires_grad = False\n",
    "\n",
    "    # Optimizer\n",
    "    nbs = 64  # nominal batch size\n",
    "    accumulate = max(\n",
    "        round(nbs / total_batch_size), 1\n",
    "    )  # accumulate loss before optimizing\n",
    "    hyp[\"weight_decay\"] *= total_batch_size * accumulate / nbs  # scale weight_decay\n",
    "    logger.info(f\"Scaled weight_decay = {hyp['weight_decay']}\")\n",
    "\n",
    "    pg0, pg1, pg2 = [], [], []  # optimizer parameter groups\n",
    "    for k, v in model.named_modules():\n",
    "        if hasattr(v, \"bias\") and isinstance(v.bias, nn.Parameter):\n",
    "            pg2.append(v.bias)  # biases\n",
    "        if isinstance(v, nn.BatchNorm2d):\n",
    "            pg0.append(v.weight)  # no decay\n",
    "        elif hasattr(v, \"weight\") and isinstance(v.weight, nn.Parameter):\n",
    "            pg1.append(v.weight)  # apply decay\n",
    "        if hasattr(v, \"im\"):\n",
    "            if hasattr(v.im, \"implicit\"):\n",
    "                pg0.append(v.im.implicit)\n",
    "            else:\n",
    "                for iv in v.im:\n",
    "                    pg0.append(iv.implicit)\n",
    "        if hasattr(v, \"imc\"):\n",
    "            if hasattr(v.imc, \"implicit\"):\n",
    "                pg0.append(v.imc.implicit)\n",
    "            else:\n",
    "                for iv in v.imc:\n",
    "                    pg0.append(iv.implicit)\n",
    "        if hasattr(v, \"imb\"):\n",
    "            if hasattr(v.imb, \"implicit\"):\n",
    "                pg0.append(v.imb.implicit)\n",
    "            else:\n",
    "                for iv in v.imb:\n",
    "                    pg0.append(iv.implicit)\n",
    "        if hasattr(v, \"imo\"):\n",
    "            if hasattr(v.imo, \"implicit\"):\n",
    "                pg0.append(v.imo.implicit)\n",
    "            else:\n",
    "                for iv in v.imo:\n",
    "                    pg0.append(iv.implicit)\n",
    "        if hasattr(v, \"ia\"):\n",
    "            if hasattr(v.ia, \"implicit\"):\n",
    "                pg0.append(v.ia.implicit)\n",
    "            else:\n",
    "                for iv in v.ia:\n",
    "                    pg0.append(iv.implicit)\n",
    "        if hasattr(v, \"attn\"):\n",
    "            if hasattr(v.attn, \"logit_scale\"):\n",
    "                pg0.append(v.attn.logit_scale)\n",
    "            if hasattr(v.attn, \"q_bias\"):\n",
    "                pg0.append(v.attn.q_bias)\n",
    "            if hasattr(v.attn, \"v_bias\"):\n",
    "                pg0.append(v.attn.v_bias)\n",
    "            if hasattr(v.attn, \"relative_position_bias_table\"):\n",
    "                pg0.append(v.attn.relative_position_bias_table)\n",
    "        if hasattr(v, \"rbr_dense\"):\n",
    "            if hasattr(v.rbr_dense, \"weight_rbr_origin\"):\n",
    "                pg0.append(v.rbr_dense.weight_rbr_origin)\n",
    "            if hasattr(v.rbr_dense, \"weight_rbr_avg_conv\"):\n",
    "                pg0.append(v.rbr_dense.weight_rbr_avg_conv)\n",
    "            if hasattr(v.rbr_dense, \"weight_rbr_pfir_conv\"):\n",
    "                pg0.append(v.rbr_dense.weight_rbr_pfir_conv)\n",
    "            if hasattr(v.rbr_dense, \"weight_rbr_1x1_kxk_idconv1\"):\n",
    "                pg0.append(v.rbr_dense.weight_rbr_1x1_kxk_idconv1)\n",
    "            if hasattr(v.rbr_dense, \"weight_rbr_1x1_kxk_conv2\"):\n",
    "                pg0.append(v.rbr_dense.weight_rbr_1x1_kxk_conv2)\n",
    "            if hasattr(v.rbr_dense, \"weight_rbr_gconv_dw\"):\n",
    "                pg0.append(v.rbr_dense.weight_rbr_gconv_dw)\n",
    "            if hasattr(v.rbr_dense, \"weight_rbr_gconv_pw\"):\n",
    "                pg0.append(v.rbr_dense.weight_rbr_gconv_pw)\n",
    "            if hasattr(v.rbr_dense, \"vector\"):\n",
    "                pg0.append(v.rbr_dense.vector)\n",
    "\n",
    "    if opt.adam:\n",
    "        optimizer = optim.Adam(\n",
    "            pg0, lr=hyp[\"lr0\"], betas=(hyp[\"momentum\"], 0.999)\n",
    "        )  # adjust beta1 to momentum\n",
    "    else:\n",
    "        optimizer = optim.SGD(\n",
    "            pg0, lr=hyp[\"lr0\"], momentum=hyp[\"momentum\"], nesterov=True\n",
    "        )\n",
    "\n",
    "    optimizer.add_param_group(\n",
    "        {\"params\": pg1, \"weight_decay\": hyp[\"weight_decay\"]}\n",
    "    )  # add pg1 with weight_decay\n",
    "    optimizer.add_param_group({\"params\": pg2})  # add pg2 (biases)\n",
    "    logger.info(\n",
    "        \"Optimizer groups: %g .bias, %g conv.weight, %g other\"\n",
    "        % (len(pg2), len(pg1), len(pg0))\n",
    "    )\n",
    "    del pg0, pg1, pg2\n",
    "\n",
    "    # Scheduler https://arxiv.org/pdf/1812.01187.pdf\n",
    "    # https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#OneCycleLR\n",
    "    if opt.linear_lr:\n",
    "        lf = (\n",
    "            lambda x: (1 - x / (epochs - 1)) * (1.0 - hyp[\"lrf\"]) + hyp[\"lrf\"]\n",
    "        )  # linear\n",
    "    else:\n",
    "        lf = one_cycle(1, hyp[\"lrf\"], epochs)  # cosine 1->hyp['lrf']\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n",
    "    # plot_lr_scheduler(optimizer, scheduler, epochs)\n",
    "\n",
    "    # EMA\n",
    "    ema = ModelEMA(model) if rank in [-1, 0] else None\n",
    "\n",
    "    # Resume\n",
    "    start_epoch, best_fitness = 0, 0.0\n",
    "    if pretrained:\n",
    "        # Optimizer\n",
    "        if ckpt[\"optimizer\"] is not None:\n",
    "            optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "            best_fitness = ckpt[\"best_fitness\"]\n",
    "\n",
    "        # EMA\n",
    "        if ema and ckpt.get(\"ema\"):\n",
    "            ema.ema.load_state_dict(ckpt[\"ema\"].float().state_dict())\n",
    "            ema.updates = ckpt[\"updates\"]\n",
    "\n",
    "        # Results\n",
    "        if ckpt.get(\"training_results\") is not None:\n",
    "            results_file.write_text(ckpt[\"training_results\"])  # write results.txt\n",
    "\n",
    "        # Epochs\n",
    "        start_epoch = ckpt[\"epoch\"] + 1\n",
    "        if opt.resume:\n",
    "            assert start_epoch > 0, (\n",
    "                \"%s training to %g epochs is finished, nothing to resume.\"\n",
    "                % (\n",
    "                    weights,\n",
    "                    epochs,\n",
    "                )\n",
    "            )\n",
    "        if epochs < start_epoch:\n",
    "            logger.info(\n",
    "                \"%s has been trained for %g epochs. Fine-tuning for %g additional epochs.\"\n",
    "                % (weights, ckpt[\"epoch\"], epochs)\n",
    "            )\n",
    "            epochs += ckpt[\"epoch\"]  # finetune additional epochs\n",
    "\n",
    "        del ckpt, state_dict\n",
    "\n",
    "    # Image sizes\n",
    "    gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
    "    nl = model.model[-1].nl  # number of detection layers (used for scaling hyp['obj'])\n",
    "    imgsz, imgsz_test = [\n",
    "        check_img_size(x, gs) for x in opt.img_size\n",
    "    ]  # verify imgsz are gs-multiples\n",
    "\n",
    "    # DP mode\n",
    "    if cuda and rank == -1 and torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # SyncBatchNorm\n",
    "    if opt.sync_bn and cuda and rank != -1:\n",
    "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)\n",
    "        logger.info(\"Using SyncBatchNorm()\")\n",
    "\n",
    "    # Train loader\n",
    "    dataloader, dataset = create_dataloader(\n",
    "        train_path,\n",
    "        imgsz,\n",
    "        batch_size,\n",
    "        gs,\n",
    "        opt,\n",
    "        hyp=hyp,\n",
    "        augment=True,\n",
    "        cache=opt.cache_images,\n",
    "        rect=opt.rect,\n",
    "        rank=rank,\n",
    "        world_size=opt.world_size,\n",
    "        workers=opt.workers,\n",
    "        image_weights=opt.image_weights,\n",
    "        quad=opt.quad,\n",
    "        prefix=colorstr(\"train: \"),\n",
    "    )\n",
    "    mlc = np.concatenate(dataset.labels, 0)[:, 0].max()  # max label class\n",
    "    nb = len(dataloader)  # number of batches\n",
    "    assert mlc < nc, (\n",
    "        \"Label class %g exceeds nc=%g in %s. Possible class labels are 0-%g\"\n",
    "        % (\n",
    "            mlc,\n",
    "            nc,\n",
    "            opt.data,\n",
    "            nc - 1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Process 0\n",
    "    if rank in [-1, 0]:\n",
    "        testloader = create_dataloader(\n",
    "            test_path,\n",
    "            imgsz_test,\n",
    "            batch_size * 2,\n",
    "            gs,\n",
    "            opt,  # testloader\n",
    "            hyp=hyp,\n",
    "            cache=opt.cache_images and not opt.notest,\n",
    "            rect=True,\n",
    "            rank=-1,\n",
    "            world_size=opt.world_size,\n",
    "            workers=opt.workers,\n",
    "            pad=0.5,\n",
    "            prefix=colorstr(\"val: \"),\n",
    "        )[0]\n",
    "\n",
    "        if not opt.resume:\n",
    "            labels = np.concatenate(dataset.labels, 0)\n",
    "            c = torch.tensor(labels[:, 0])  # classes\n",
    "            # cf = torch.bincount(c.long(), minlength=nc) + 1.  # frequency\n",
    "            # model._initialize_biases(cf.to(device))\n",
    "            if plots:\n",
    "                # plot_labels(labels, names, save_dir, loggers)\n",
    "                if tb_writer:\n",
    "                    tb_writer.add_histogram(\"classes\", c, 0)\n",
    "\n",
    "            # Anchors\n",
    "            if not opt.noautoanchor:\n",
    "                check_anchors(dataset, model=model, thr=hyp[\"anchor_t\"], imgsz=imgsz)\n",
    "            model.half().float()  # pre-reduce anchor precision\n",
    "\n",
    "    # DDP mode\n",
    "    if cuda and rank != -1:\n",
    "        model = DDP(\n",
    "            model,\n",
    "            device_ids=[opt.local_rank],\n",
    "            output_device=opt.local_rank,\n",
    "            # nn.MultiheadAttention incompatibility with DDP https://github.com/pytorch/pytorch/issues/26698\n",
    "            find_unused_parameters=any(\n",
    "                isinstance(layer, nn.MultiheadAttention) for layer in model.modules()\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Model parameters\n",
    "    hyp[\"box\"] *= 3.0 / nl  # scale to layers\n",
    "    hyp[\"cls\"] *= nc / 80.0 * 3.0 / nl  # scale to classes and layers\n",
    "    hyp[\"obj\"] *= (imgsz / 640) ** 2 * 3.0 / nl  # scale to image size and layers\n",
    "    hyp[\"label_smoothing\"] = opt.label_smoothing\n",
    "    model.nc = nc  # attach number of classes to model\n",
    "    model.hyp = hyp  # attach hyperparameters to model\n",
    "    model.gr = 1.0  # iou loss ratio (obj_loss = 1.0 or iou)\n",
    "    model.class_weights = (\n",
    "        labels_to_class_weights(dataset.labels, nc).to(device) * nc\n",
    "    )  # attach class weights\n",
    "    model.names = names\n",
    "\n",
    "    # Start training\n",
    "    t0 = time.time()\n",
    "    nw = max(\n",
    "        round(hyp[\"warmup_epochs\"] * nb), 1000\n",
    "    )  # number of warmup iterations, max(3 epochs, 1k iterations)\n",
    "    # nw = min(nw, (epochs - start_epoch) / 2 * nb)  # limit warmup to < 1/2 of training\n",
    "    maps = np.zeros(nc)  # mAP per class\n",
    "    results = (\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "    )  # P, R, mAP@.5, mAP@.5-.95, val_loss(box, obj, cls)\n",
    "    scheduler.last_epoch = start_epoch - 1  # do not move\n",
    "    scaler = amp.GradScaler(enabled=cuda)\n",
    "    compute_loss_ota = ComputeLossOTA(model)  # init loss class\n",
    "    compute_loss = ComputeLoss(model)  # init loss class\n",
    "    logger.info(\n",
    "        f\"Image sizes {imgsz} train, {imgsz_test} test\\n\"\n",
    "        f\"Using {dataloader.num_workers} dataloader workers\\n\"\n",
    "        f\"Logging results to {save_dir}\\n\"\n",
    "        f\"Starting training for {epochs} epochs...\"\n",
    "    )\n",
    "    torch.save(model, wdir / \"init.pt\")\n",
    "    for epoch in range(\n",
    "        start_epoch, epochs\n",
    "    ):  # epoch ------------------------------------------------------------------\n",
    "        model.train()\n",
    "\n",
    "        # print(f\"DEBUG: {epoch=} / image weights\")\n",
    "        # Update image weights (optional)\n",
    "        if opt.image_weights:\n",
    "            # Generate indices\n",
    "            if rank in [-1, 0]:\n",
    "                cw = (\n",
    "                    model.class_weights.cpu().numpy() * (1 - maps) ** 2 / nc\n",
    "                )  # class weights\n",
    "                iw = labels_to_image_weights(\n",
    "                    dataset.labels, nc=nc, class_weights=cw\n",
    "                )  # image weights\n",
    "                dataset.indices = random.choices(\n",
    "                    range(dataset.n), weights=iw, k=dataset.n\n",
    "                )  # rand weighted idx\n",
    "            # Broadcast if DDP\n",
    "            if rank != -1:\n",
    "                indices = (\n",
    "                    torch.tensor(dataset.indices)\n",
    "                    if rank == 0\n",
    "                    else torch.zeros(dataset.n)\n",
    "                ).int()\n",
    "                dist.broadcast(indices, 0)\n",
    "                if rank != 0:\n",
    "                    dataset.indices = indices.cpu().numpy()\n",
    "\n",
    "        # Update mosaic border\n",
    "        # b = int(random.uniform(0.25 * imgsz, 0.75 * imgsz + gs) // gs * gs)\n",
    "        # dataset.mosaic_border = [b - imgsz, -b]  # height, width borders\n",
    "\n",
    "        mloss = torch.zeros(4, device=device)  # mean losses\n",
    "        if rank != -1:\n",
    "            dataloader.sampler.set_epoch(epoch)\n",
    "        pbar = enumerate(dataloader)\n",
    "        logger.info(\n",
    "            (\"\\n\" + \"%10s\" * 8)\n",
    "            % (\n",
    "                \"Epoch\",\n",
    "                \"gpu_mem\",\n",
    "                \"box\",\n",
    "                \"obj\",\n",
    "                \"cls\",\n",
    "                \"total\",\n",
    "                \"labels\",\n",
    "                \"img_size\",\n",
    "            )\n",
    "        )\n",
    "        if rank in [-1, 0]:\n",
    "            pbar = tqdm(pbar, total=nb)  # progress bar\n",
    "            ...\n",
    "        optimizer.zero_grad()\n",
    "        for (\n",
    "            i,\n",
    "            (\n",
    "                imgs,\n",
    "                targets,\n",
    "                paths,\n",
    "                _,\n",
    "            ),\n",
    "        ) in (\n",
    "            pbar\n",
    "        ):  # batch -------------------------------------------------------------\n",
    "            ni = i + nb * epoch  # number integrated batches (since train start)\n",
    "            imgs = (\n",
    "                imgs.to(device, non_blocking=True).float() / 255.0\n",
    "            )  # uint8 to float32, 0-255 to 0.0-1.0\n",
    "\n",
    "            # Warmup\n",
    "            if ni <= nw:\n",
    "                # print(f\"DEBUG: {epoch=} / {i=} / warmup\")\n",
    "                xi = [0, nw]  # x interp\n",
    "                # model.gr = np.interp(ni, xi, [0.0, 1.0])  # iou loss ratio (obj_loss = 1.0 or iou)\n",
    "                accumulate = max(\n",
    "                    1, np.interp(ni, xi, [1, nbs / total_batch_size]).round()\n",
    "                )\n",
    "                for j, x in enumerate(optimizer.param_groups):\n",
    "                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n",
    "                    x[\"lr\"] = np.interp(\n",
    "                        ni,\n",
    "                        xi,\n",
    "                        [\n",
    "                            hyp[\"warmup_bias_lr\"] if j == 2 else 0.0,\n",
    "                            x[\"initial_lr\"] * lf(epoch),\n",
    "                        ],\n",
    "                    )\n",
    "                    if \"momentum\" in x:\n",
    "                        x[\"momentum\"] = np.interp(\n",
    "                            ni, xi, [hyp[\"warmup_momentum\"], hyp[\"momentum\"]]\n",
    "                        )\n",
    "\n",
    "            # Multi-scale\n",
    "            if opt.multi_scale:\n",
    "                # print(f\"DEBUG: {epoch=} / {i=} / multi-scale\")\n",
    "                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size\n",
    "                sf = sz / max(imgs.shape[2:])  # scale factor\n",
    "                if sf != 1:\n",
    "                    ns = [\n",
    "                        math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]\n",
    "                    ]  # new shape (stretched to gs-multiple)\n",
    "                    imgs = F.interpolate(\n",
    "                        imgs, size=ns, mode=\"bilinear\", align_corners=False\n",
    "                    )\n",
    "\n",
    "            # Forward\n",
    "            with amp.autocast(enabled=cuda):\n",
    "                # print(f\"DEBUG: {epoch=} / {i=} / forward / {imgs.shape=}\")\n",
    "                pred = model(imgs)  # forward\n",
    "                if \"loss_ota\" not in hyp or hyp[\"loss_ota\"] == 1:\n",
    "                    # print(f\"DEBUG: {epoch=} / {i=} / loss OTA\")\n",
    "                    loss, loss_items = compute_loss_ota(\n",
    "                        pred, targets.to(device), imgs\n",
    "                    )  # loss scaled by batch_size\n",
    "                else:\n",
    "                    # print(f\"DEBUG: {epoch=} / {i=} / loss\")\n",
    "                    loss, loss_items = compute_loss(\n",
    "                        pred, targets.to(device)\n",
    "                    )  # loss scaled by batch_size\n",
    "                if rank != -1:\n",
    "                    loss *= (\n",
    "                        opt.world_size\n",
    "                    )  # gradient averaged between devices in DDP mode\n",
    "                if opt.quad:\n",
    "                    loss *= 4.0\n",
    "\n",
    "            # Backward\n",
    "            # print(f\"DEBUG: {epoch=} / {i=} / backward\")\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Optimize\n",
    "            if ni % accumulate == 0:\n",
    "                # print(f\"DEBUG: {epoch=} / {i=} / optimize\")\n",
    "                scaler.step(optimizer)  # optimizer.step\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                if ema:\n",
    "                    ema.update(model)\n",
    "\n",
    "            # Print\n",
    "            if rank in [-1, 0]:\n",
    "                # print(f\"DEBUG: {epoch=} / {i=} / loss\")\n",
    "                mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses\n",
    "                mem = \"%.3gG\" % (\n",
    "                    torch.cuda.memory_reserved() / 1e9\n",
    "                    if torch.cuda.is_available()\n",
    "                    else 0\n",
    "                )  # (GB)\n",
    "                s = (\"%10s\" * 2 + \"%10.4g\" * 6) % (\n",
    "                    \"%g/%g\" % (epoch, epochs - 1),\n",
    "                    mem,\n",
    "                    *mloss,\n",
    "                    targets.shape[0],\n",
    "                    imgs.shape[-1],\n",
    "                )\n",
    "                if hasattr(pbar, \"set_description\"):\n",
    "                    pbar.set_description(s)\n",
    "\n",
    "                # Plot\n",
    "                # print(f\"DEBUG: {epoch=} / {i=} / plot\")\n",
    "                if plots and ni < 10:\n",
    "                    f = save_dir / f\"train_batch{ni}.jpg\"  # filename\n",
    "                    Thread(\n",
    "                        target=plot_images,\n",
    "                        args=(imgs, targets, paths, f),\n",
    "                        daemon=True,\n",
    "                    ).start()\n",
    "                    # if tb_writer:\n",
    "                    #     tb_writer.add_image(f, result, dataformats='HWC', global_step=epoch)\n",
    "                    #     tb_writer.add_graph(torch.jit.trace(model, imgs, strict=False), [])  # add model graph\n",
    "                elif plots and ni == 10 and wandb_logger.wandb:\n",
    "                    wandb_logger.log(\n",
    "                        {\n",
    "                            \"Mosaics\": [\n",
    "                                wandb_logger.wandb.Image(str(x), caption=x.name)\n",
    "                                for x in save_dir.glob(\"train*.jpg\")\n",
    "                                if x.exists()\n",
    "                            ]\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            # end batch ------------------------------------------------------------------------------------------------\n",
    "        # end epoch ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Scheduler\n",
    "        # print(f\"DEBUG: {epoch=} / scheduler\")\n",
    "        lr = [x[\"lr\"] for x in optimizer.param_groups]  # for tensorboard\n",
    "        scheduler.step()\n",
    "\n",
    "        # DDP process 0 or single-GPU\n",
    "        if rank in [-1, 0]:\n",
    "            # mAP\n",
    "            # print(f\"DEBUG: {epoch=} / mAP -->\")\n",
    "            ema.update_attr(\n",
    "                model,\n",
    "                include=[\n",
    "                    \"yaml\",\n",
    "                    \"nc\",\n",
    "                    \"hyp\",\n",
    "                    \"gr\",\n",
    "                    \"names\",\n",
    "                    \"stride\",\n",
    "                    \"class_weights\",\n",
    "                ],\n",
    "            )\n",
    "            final_epoch = epoch + 1 == epochs\n",
    "            if not opt.notest or final_epoch:  # Calculate mAP\n",
    "                wandb_logger.current_epoch = epoch + 1\n",
    "                results, maps, times = test(\n",
    "                    data_dict,\n",
    "                    batch_size=batch_size * 2,\n",
    "                    imgsz=imgsz_test,\n",
    "                    model=ema.ema,\n",
    "                    single_cls=opt.single_cls,\n",
    "                    dataloader=testloader,\n",
    "                    save_dir=save_dir,\n",
    "                    verbose=nc < 50 and final_epoch,\n",
    "                    plots=plots and final_epoch,\n",
    "                    wandb_logger=wandb_logger,\n",
    "                    compute_loss=compute_loss,\n",
    "                    is_coco=is_coco,\n",
    "                    v5_metric=opt.v5_metric,\n",
    "                )\n",
    "\n",
    "            # Write\n",
    "            # print(f\"DEBUG: {epoch=} / write + log + save\")\n",
    "            with open(results_file, \"a\") as f:\n",
    "                f.write(s + \"%10.4g\" * 7 % results + \"\\n\")  # append metrics, val_loss\n",
    "            if len(opt.name) and opt.bucket:\n",
    "                os.system(\n",
    "                    \"gsutil cp %s gs://%s/results/results%s.txt\"\n",
    "                    % (results_file, opt.bucket, opt.name)\n",
    "                )\n",
    "\n",
    "            # Log\n",
    "            tags = [\n",
    "                \"train/box_loss\",\n",
    "                \"train/obj_loss\",\n",
    "                \"train/cls_loss\",  # train loss\n",
    "                \"metrics/precision\",\n",
    "                \"metrics/recall\",\n",
    "                \"metrics/mAP_0.5\",\n",
    "                \"metrics/mAP_0.5:0.95\",\n",
    "                \"val/box_loss\",\n",
    "                \"val/obj_loss\",\n",
    "                \"val/cls_loss\",  # val loss\n",
    "                \"x/lr0\",\n",
    "                \"x/lr1\",\n",
    "                \"x/lr2\",\n",
    "            ]  # params\n",
    "            for x, tag in zip(list(mloss[:-1]) + list(results) + lr, tags):\n",
    "                if tb_writer:\n",
    "                    tb_writer.add_scalar(tag, x, epoch)  # tensorboard\n",
    "                if wandb_logger.wandb:\n",
    "                    wandb_logger.log({tag: x})  # W&B\n",
    "\n",
    "            # Update best mAP\n",
    "            fi = fitness(\n",
    "                np.array(results).reshape(1, -1)\n",
    "            )  # weighted combination of [P, R, mAP@.5, mAP@.5-.95]\n",
    "            if fi > best_fitness:\n",
    "                best_fitness = fi\n",
    "            wandb_logger.end_epoch(best_result=best_fitness == fi)\n",
    "\n",
    "            # Save model\n",
    "            if (not opt.nosave) or (final_epoch and not opt.evolve):  # if save\n",
    "                ckpt = {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"best_fitness\": best_fitness,\n",
    "                    \"training_results\": results_file.read_text(),\n",
    "                    \"model\": deepcopy(\n",
    "                        model.module if is_parallel(model) else model\n",
    "                    ).half(),\n",
    "                    \"ema\": deepcopy(ema.ema).half(),\n",
    "                    \"updates\": ema.updates,\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"wandb_id\": (\n",
    "                        wandb_logger.wandb_run.id if wandb_logger.wandb else None\n",
    "                    ),\n",
    "                }\n",
    "\n",
    "                # Save last, best and delete\n",
    "                torch.save(ckpt, last)\n",
    "                if best_fitness == fi:\n",
    "                    torch.save(ckpt, best)\n",
    "                if (best_fitness == fi) and (epoch >= 200):\n",
    "                    torch.save(ckpt, wdir / \"best_{:03d}.pt\".format(epoch))\n",
    "                if epoch == 0:\n",
    "                    torch.save(ckpt, wdir / \"epoch_{:03d}.pt\".format(epoch))\n",
    "                elif ((epoch + 1) % 25) == 0:\n",
    "                    torch.save(ckpt, wdir / \"epoch_{:03d}.pt\".format(epoch))\n",
    "                elif epoch >= (epochs - 5):\n",
    "                    torch.save(ckpt, wdir / \"epoch_{:03d}.pt\".format(epoch))\n",
    "                if wandb_logger.wandb:\n",
    "                    if (\n",
    "                        (epoch + 1) % opt.save_period == 0 and not final_epoch\n",
    "                    ) and opt.save_period != -1:\n",
    "                        wandb_logger.log_model(\n",
    "                            last.parent,\n",
    "                            opt,\n",
    "                            epoch,\n",
    "                            fi,\n",
    "                            best_model=best_fitness == fi,\n",
    "                        )\n",
    "                del ckpt\n",
    "\n",
    "        # end epoch ----------------------------------------------------------------------------------------------------\n",
    "    # end training\n",
    "    # print(f\"DEBUG: {epoch=} / END --> plots / final\")\n",
    "    if rank in [-1, 0]:\n",
    "        # Plots\n",
    "        if plots:\n",
    "            plot_results(save_dir=save_dir)  # save as results.png\n",
    "            if wandb_logger.wandb:\n",
    "                files = [\n",
    "                    \"results.png\",\n",
    "                    \"confusion_matrix.png\",\n",
    "                    *[f\"{x}_curve.png\" for x in (\"F1\", \"PR\", \"P\", \"R\")],\n",
    "                ]\n",
    "                wandb_logger.log(\n",
    "                    {\n",
    "                        \"Results\": [\n",
    "                            wandb_logger.wandb.Image(str(save_dir / f), caption=f)\n",
    "                            for f in files\n",
    "                            if (save_dir / f).exists()\n",
    "                        ]\n",
    "                    }\n",
    "                )\n",
    "        # Test best.pt\n",
    "        logger.info(\n",
    "            \"%g epochs completed in %.3f hours.\\n\"\n",
    "            % (epoch - start_epoch + 1, (time.time() - t0) / 3600)\n",
    "        )\n",
    "        if opt.data.endswith(\"coco.yaml\") and nc == 80:  # if COCO\n",
    "            for m in (last, best) if best.exists() else (last):  # speed, mAP tests\n",
    "                results, _, _ = test.test(\n",
    "                    opt.data,\n",
    "                    batch_size=batch_size * 2,\n",
    "                    imgsz=imgsz_test,\n",
    "                    conf_thres=0.001,\n",
    "                    iou_thres=0.7,\n",
    "                    model=attempt_load(m, device).half(),\n",
    "                    single_cls=opt.single_cls,\n",
    "                    dataloader=testloader,\n",
    "                    save_dir=save_dir,\n",
    "                    save_json=True,\n",
    "                    plots=False,\n",
    "                    is_coco=is_coco,\n",
    "                    v5_metric=opt.v5_metric,\n",
    "                )\n",
    "\n",
    "        # Strip optimizers\n",
    "        final = best if best.exists() else last  # final model\n",
    "        for f in last, best:\n",
    "            if f.exists():\n",
    "                strip_optimizer(f)  # strip optimizers\n",
    "        if opt.bucket:\n",
    "            os.system(f\"gsutil cp {final} gs://{opt.bucket}/weights\")  # upload\n",
    "        if wandb_logger.wandb and not opt.evolve:  # Log the stripped model\n",
    "            wandb_logger.wandb.log_artifact(\n",
    "                str(final),\n",
    "                type=\"model\",\n",
    "                name=\"run_\" + wandb_logger.wandb_run.id + \"_model\",\n",
    "                aliases=[\"last\", \"best\", \"stripped\"],\n",
    "            )\n",
    "        wandb_logger.finish_run()\n",
    "    else:\n",
    "        dist.destroy_process_group()\n",
    "    torch.cuda.empty_cache()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3727ea88-19e1-439f-8a99-8c84a7c81644",
   "metadata": {},
   "source": [
    "# Config\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "CLI scripts use model and hyperparameters configuration files. Hyperparameters configuration is the one to be overriden.\n",
    "\n",
    "This file contains training parameters and augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b65fb58-b85d-49d2-aa26-b05f21c680c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from yaml import dump\n",
    "\n",
    "\n",
    "hyp = {\n",
    "    \"lr0\": 0.001,\n",
    "    \"lrf\": 0.25,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"warmup_epochs\": 0.01,\n",
    "    \"warmup_momentum\": 0.8,\n",
    "    \"warmup_bias_lr\": 0.1,\n",
    "    \"box\": 0.35,\n",
    "    \"cls\": 0.5,\n",
    "    \"cls_pw\": 1.0,\n",
    "    \"obj\": 1.0,\n",
    "    \"obj_pw\": 2.0,\n",
    "    \"iou_t\": 0.2,  # superseeded with 'anchor_t'\n",
    "    \"anchor_t\": 4.0,\n",
    "    \"fl_gamma\": 0.0,\n",
    "    \"hsv_h\": 0.015,\n",
    "    \"hsv_s\": 0.65,\n",
    "    \"hsv_v\": 0.35,\n",
    "    \"degrees\": 30.0,\n",
    "    \"translate\": 0.1,\n",
    "    \"scale\": 0.5,\n",
    "    \"shear\": 10.0,\n",
    "    \"perspective\": 0.000001,\n",
    "    \"flipud\": 0.0,\n",
    "    \"fliplr\": 0.5,\n",
    "    \"mosaic\": 0.65,\n",
    "    \"mixup\": 0.0,  # label smoothing\n",
    "    \"copy_paste\": 0.0,  # instance segmentation\n",
    "    \"paste_in\": 0.0,  # cutout augmentation\n",
    "    \"loss_ota\": 0,\n",
    "}\n",
    "\n",
    "with open(\"hyp.yaml\", \"w\") as hypfile:\n",
    "    dump(hyp, hypfile, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699289d9-0007-491e-bb49-07048fe6498e",
   "metadata": {},
   "source": [
    "## Options\n",
    "\n",
    "CLI script uses Python's ArgumentParser for training configuration. ArgumentParser do not work well with Jupyter notebooks, so a mock object with the same fields is required.\n",
    "\n",
    "Dataclass object is used as such a mock parameters object.\n",
    "\n",
    "Three fields of the `opt` mock object point to external config files:\n",
    "* `opt.cfg` - model architecture config file (empty);\n",
    "* `opt.data` - dataset config file;\n",
    "* `opt.hyp` - hyperparameters file (created by the cell above).\n",
    "\n",
    "> The `opt.cfg` field is empty for training resume. Fine-tuning pretrained model is kinda resuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c644a56-ab53-4467-877b-4cfeed2c53de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from os import getenv\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    weights: str = \"yolov7-tiny.pt\"\n",
    "    cfg: str = \"\"  # (default: '')\n",
    "    data: str = \"data/config/Pelicans.yaml\"\n",
    "    # hyp: str = 'runs/train/evolve/hyp_evolved.yaml'\n",
    "    # hyp: str = 'data/hyp.scratch.tiny.yaml'\n",
    "    hyp: str = \"hyp.yaml\"\n",
    "    epochs: int = 1500  # total epochs (default: 300)\n",
    "    batch_size: int = 2  # training batch size (default: 16)\n",
    "    total_batch_size: int = 1  # (stub, calculated)\n",
    "    img_size: int = 3840  # input shape (value or list [w, h], default: 640)\n",
    "    rect: bool = False  # (default: False)\n",
    "    resume: bool = False  # (default: False)\n",
    "    nosave: bool = False  # do not save checkpoints (default: False)\n",
    "    notest: bool = False  # do not test (default: False)\n",
    "    noautoanchor: bool = False\n",
    "    evolve: bool = False  # run HPO with evolution algorithm (default: False)\n",
    "    bucket: str = \"\"  # (default: '')\n",
    "    cache_images: bool = False  # (default: False)\n",
    "    image_weights: bool = False\n",
    "    device: str = \"cpu\"  # target device (default: 'cuda')\n",
    "    multi_scale: bool = False  # random shapes (BoF, default: False)\n",
    "    single_cls: bool = (\n",
    "        False  # squeeze all non-background classes to 'item' (default: False)\n",
    "    )\n",
    "    adam: bool = True  # use Adam optimizer instead of SGD (default: False)\n",
    "    sync_bn: bool = False  # SyncBatchNorm (CUDA only, master node, BoF, default: False)\n",
    "    local_rank: int = -1  # (stub)\n",
    "    workers: int = 1  # data loader workers (default: 2)\n",
    "    project: str = \"runs/train\"  # W&B experiment name\n",
    "    entity: str = None  # W&B entity (default: None)\n",
    "    name: str = \"exp\"  # experiment base name (default: 'exp')\n",
    "    exist_ok: bool = False  # overwrite experiment (default: False)\n",
    "    quad: bool = False  # x4 loss value (default: False)\n",
    "    linear_lr: bool = False  # linear vs cosine LR scheduler (BoF, default: False)\n",
    "    label_smoothing: float = 0.05  # BCE loss class label smoothing (BoF, default: 0.0)\n",
    "    upload_dataset: bool = False  # W&B save dataset as artifact (default: False)\n",
    "    bbox_interval: int = -1  # W&B bbox image logging interval (default: -1)\n",
    "    save_period: int = -1  # log model every n epoch (default: -1)\n",
    "    artifact_alias: str = \"latest\"  # (default: 'latest')\n",
    "    freeze: int = 0  # list of layers to freeze (default: 0) FIXME: must be List[int]\n",
    "    v5_metric: bool = False  # use YOLOv5 metrics (default: False)\n",
    "    world_size: int = int(getenv(\"WORLD_SIZE\", \"1\"))  # number of nodes (stub)\n",
    "    global_rank: int = int(getenv(\"RANK\", \"-1\"))  # current node rank (stub)\n",
    "    save_dir: str = \"\"\n",
    "\n",
    "\n",
    "opt = Config()\n",
    "# opt.img_size = [1280, 736]\n",
    "# opt.freeze = [0, 1, 2, 3, 4, 5]\n",
    "opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ce5199-3b05-45b2-bd14-ff1b194cf819",
   "metadata": {},
   "source": [
    "# Run loop\n",
    "\n",
    "This is the only model train loop.\n",
    "\n",
    "Depending on `opt.evolve` parameter set it can switch between HPO (genetic algorithm) and training.\n",
    "\n",
    "* HPO is used for generating optimal hyperparameter config.\n",
    "\n",
    "* Training uses either the generated hyperparameters config or a hyperparameters config defined manually.\n",
    "\n",
    "This piece of code is taken from the [train.py](https://github.com/ValV/yolov7/blob/master/train.py) as well as the `train` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23654502-4aec-49f7-8e59-d66d9d9e47b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_logging(opt.global_rank)\n",
    "# if opt.global_rank in [-1, 0]:\n",
    "#    check_git_status()\n",
    "#    check_requirements()\n",
    "\n",
    "opt.freeze = opt.freeze if isinstance(opt.freeze, list) else [int(opt.freeze)]\n",
    "\n",
    "# Resume\n",
    "wandb_run = check_wandb_resume(opt)\n",
    "if opt.resume and not wandb_run:  # resume an interrupted run\n",
    "    ckpt = (\n",
    "        opt.resume if isinstance(opt.resume, str) else get_latest_run()\n",
    "    )  # specified or most recent path\n",
    "    assert os.path.isfile(ckpt), \"ERROR: --resume checkpoint does not exist\"\n",
    "    apriori = opt.global_rank, opt.local_rank\n",
    "    with open(Path(ckpt).parent.parent / \"opt.yaml\") as f:\n",
    "        opt = Config(**yaml.load(f, Loader=yaml.SafeLoader))  # replace\n",
    "    (\n",
    "        opt.cfg,\n",
    "        opt.weights,\n",
    "        opt.resume,\n",
    "        opt.batch_size,\n",
    "        opt.global_rank,\n",
    "        opt.local_rank,\n",
    "    ) = (\n",
    "        \"\",\n",
    "        ckpt,\n",
    "        True,\n",
    "        opt.total_batch_size,\n",
    "        *apriori,\n",
    "    )  # reinstate\n",
    "    logger.info(\"Resuming training from %s\" % ckpt)\n",
    "else:\n",
    "    # opt.hyp = opt.hyp or ('hyp.finetune.yaml' if opt.weights else 'hyp.scratch.yaml')\n",
    "    opt.data, opt.cfg, opt.hyp = (\n",
    "        check_file(opt.data),\n",
    "        check_file(opt.cfg),\n",
    "        check_file(opt.hyp),\n",
    "    )  # check files\n",
    "    assert len(opt.cfg) or len(\n",
    "        opt.weights\n",
    "    ), \"either --cfg or --weights must be specified\"\n",
    "    opt.img_size = opt.img_size if isinstance(opt.img_size, list) else [opt.img_size]\n",
    "    opt.img_size.extend(\n",
    "        [opt.img_size[-1]] * (2 - len(opt.img_size))\n",
    "    )  # extend to 2 sizes (train, test)\n",
    "    opt.name = \"evolve\" if opt.evolve else opt.name\n",
    "    opt.save_dir = increment_path(\n",
    "        Path(opt.project) / opt.name, exist_ok=opt.exist_ok | opt.evolve\n",
    "    )  # increment run\n",
    "\n",
    "# DDP mode\n",
    "opt.total_batch_size = opt.batch_size\n",
    "device = select_device(opt.device, batch_size=opt.batch_size)\n",
    "if opt.local_rank != -1:\n",
    "    assert torch.cuda.device_count() > opt.local_rank\n",
    "    torch.cuda.set_device(opt.local_rank)\n",
    "    device = torch.device(\"cuda\", opt.local_rank)\n",
    "    dist.init_process_group(backend=\"nccl\", init_method=\"env://\")  # distributed backend\n",
    "    assert (\n",
    "        opt.batch_size % opt.world_size == 0\n",
    "    ), \"--batch-size must be multiple of CUDA device count\"\n",
    "    opt.batch_size = opt.total_batch_size // opt.world_size\n",
    "\n",
    "# Hyperparameters\n",
    "with open(opt.hyp) as f:\n",
    "    hyp = yaml.load(f, Loader=yaml.SafeLoader)  # load hyps\n",
    "\n",
    "# Train\n",
    "logger.info(opt)\n",
    "if not opt.evolve:\n",
    "    tb_writer = None  # init loggers\n",
    "    if opt.global_rank in [-1, 0]:\n",
    "        prefix = colorstr(\"tensorboard: \")\n",
    "        logger.info(\n",
    "            f\"{prefix}Start with 'tensorboard --logdir {opt.project}', view at http://localhost:6006/\"\n",
    "        )\n",
    "        tb_writer = SummaryWriter(opt.save_dir)  # Tensorboard\n",
    "    train(hyp, opt, device, tb_writer)\n",
    "\n",
    "# Evolve hyperparameters (optional)\n",
    "else:\n",
    "    # Hyperparameter evolution metadata (mutation scale 0-1, lower_limit, upper_limit)\n",
    "    meta = {\n",
    "        # 'lr0': (1, 1e-5, 1e-1),  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
    "        \"lr0\": (0.5, 1e-6, 1e-1),  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
    "        # 'lrf': (1, 0.01, 1.0),  # final OneCycleLR learning rate (lr0 * lrf)\n",
    "        \"lrf\": (1, 0.001, 1.0),  # final OneCycleLR learning rate (lr0 * lrf)\n",
    "        \"momentum\": (0.3, 0.6, 0.98),  # SGD momentum/Adam beta1\n",
    "        \"weight_decay\": (1, 0.0, 0.001),  # optimizer weight decay\n",
    "        \"warmup_epochs\": (1, 0.0, 15.0),  # warmup epochs (fractions ok)\n",
    "        \"warmup_momentum\": (1, 0.0, 0.95),  # warmup initial momentum\n",
    "        \"warmup_bias_lr\": (1, 0.0, 0.2),  # warmup initial bias lr\n",
    "        # 'box': (1, 0.02, 0.2),  # box loss gain\n",
    "        \"box\": (1, 0.01, 1.0),  # box loss gain\n",
    "        \"cls\": (1, 0.2, 4.0),  # cls loss gain\n",
    "        \"cls_pw\": (1, 0.5, 2.0),  # cls BCELoss positive_weight\n",
    "        \"obj\": (1, 0.2, 4.0),  # obj loss gain (scale with pixels)\n",
    "        \"obj_pw\": (1, 0.5, 2.0),  # obj BCELoss positive_weight\n",
    "        # 'iou_t': (0, 0.1, 0.7),  # IoU training threshold\n",
    "        \"iou_t\": (0, 0.1, 0.9),  # IoU training threshold\n",
    "        \"anchor_t\": (1, 2.0, 8.0),  # anchor-multiple threshold\n",
    "        # 'anchors': (2, 2.0, 10.0),  # anchors per output grid (0 to ignore)\n",
    "        \"anchors\": (0, 2.0, 10.0),  # anchors per output grid (0 to ignore)\n",
    "        \"fl_gamma\": (\n",
    "            0,\n",
    "            0.0,\n",
    "            2.0,\n",
    "        ),  # focal loss gamma (efficientDet default gamma=1.5)\n",
    "        # 'hsv_h': (1, 0.0, 0.1),  # image HSV-Hue augmentation (fraction)\n",
    "        \"hsv_h\": (0, 0.0, 0.1),  # image HSV-Hue augmentation (fraction)\n",
    "        # 'hsv_s': (1, 0.0, 0.9),  # image HSV-Saturation augmentation (fraction)\n",
    "        \"hsv_s\": (0, 0.0, 0.9),  # image HSV-Saturation augmentation (fraction)\n",
    "        # 'hsv_v': (1, 0.0, 0.9),  # image HSV-Value augmentation (fraction)\n",
    "        \"hsv_v\": (0, 0.0, 0.9),  # image HSV-Value augmentation (fraction)\n",
    "        # 'degrees': (1, 0.0, 45.0),  # image rotation (+/- deg)\n",
    "        \"degrees\": (0, 0.0, 45.0),  # image rotation (+/- deg)\n",
    "        # 'translate': (1, 0.0, 0.9),  # image translation (+/- fraction)\n",
    "        \"translate\": (0, 0.0, 0.9),  # image translation (+/- fraction)\n",
    "        # 'scale': (1, 0.0, 0.9),  # image scale (+/- gain)\n",
    "        \"scale\": (0, 0.0, 0.1),  # image scale (+/- gain)\n",
    "        # 'shear': (1, 0.0, 10.0),  # image shear (+/- deg)\n",
    "        \"shear\": (0, 0.0, 10.0),  # image shear (+/- deg)\n",
    "        # 'perspective': (0, 0.0, 0.001),  # image perspective (+/- fraction), range 0-0.001\n",
    "        \"perspective\": (\n",
    "            0,\n",
    "            0.0,\n",
    "            0.001,\n",
    "        ),  # image perspective (+/- fraction), range 0-0.001\n",
    "        # 'flipud': (1, 0.0, 1.0),  # image flip up-down (probability)\n",
    "        \"flipud\": (0, 0.0, 1.0),  # image flip up-down (probability)\n",
    "        # 'fliplr': (0, 0.0, 1.0),  # image flip left-right (probability)\n",
    "        \"fliplr\": (0, 0.0, 1.0),  # image flip left-right (probability)\n",
    "        # 'mosaic': (1, 0.0, 1.0),  # image mixup (probability)\n",
    "        \"mosaic\": (0, 0.0, 1.0),  # image mixup (probability)\n",
    "        # 'mixup': (1, 0.0, 1.0),   # image mixup (probability)\n",
    "        \"mixup\": (0, 0.0, 1.0),  # image mixup (probability)\n",
    "        # 'copy_paste': (1, 0.0, 1.0),  # segment copy-paste (probability)\n",
    "        \"copy_paste\": (0, 0.0, 1.0),  # segment copy-paste (probability)\n",
    "        # 'paste_in': (1, 0.0, 1.0)     # segment copy-paste (probability)\n",
    "        \"paste_in\": (0, 0.0, 1.0),  # segment copy-paste (probability)\n",
    "    }\n",
    "\n",
    "    with open(opt.hyp, errors=\"ignore\") as f:\n",
    "        hyp = yaml.safe_load(f)  # load hyps dict\n",
    "        if \"anchors\" not in hyp:  # anchors commented in hyp.yaml\n",
    "            hyp[\"anchors\"] = 3\n",
    "\n",
    "    assert opt.local_rank == -1, \"DDP mode not implemented for --evolve\"\n",
    "    opt.notest, opt.nosave = True, True  # only test/save final epoch\n",
    "    # ei = [isinstance(x, (int, float)) for x in hyp.values()]  # evolvable indices\n",
    "    yaml_file = Path(opt.save_dir) / \"hyp_evolved.yaml\"  # save best result here\n",
    "    if opt.bucket:\n",
    "        os.system(\n",
    "            \"gsutil cp gs://%s/evolve.txt .\" % opt.bucket\n",
    "        )  # download evolve.txt if exists\n",
    "\n",
    "    for _ in range(100):  # FIXME: generations to evolve\n",
    "        if Path(\n",
    "            \"evolve.txt\"\n",
    "        ).exists():  # if evolve.txt exists: select best hyps and mutate\n",
    "            # Select parent(s)\n",
    "            parent = \"single\"  # parent selection method: 'single' or 'weighted'\n",
    "            x = np.loadtxt(\"evolve.txt\", ndmin=2)\n",
    "            n = min(5, len(x))  # number of previous results to consider\n",
    "            x = x[np.argsort(-fitness(x))][:n]  # top n mutations\n",
    "            w = fitness(x) - fitness(x).min()  # weights\n",
    "            w = w if w.any() else None\n",
    "            if parent == \"single\" or len(x) == 1:\n",
    "                # x = x[random.randint(0, n - 1)]  # random selection\n",
    "                x = x[random.choices(range(n), weights=w)[0]]  # weighted selection\n",
    "            elif parent == \"weighted\":\n",
    "                x = (x * w.reshape(n, 1)).sum(0) / w.sum()  # weighted combination\n",
    "\n",
    "            # Mutate\n",
    "            mp, s = 0.8, 0.2  # mutation probability, sigma\n",
    "            npr = np.random\n",
    "            npr.seed(int(time.time()))\n",
    "            g = np.array([x[0] for x in meta.values()])  # gains 0-1\n",
    "            ng = len(meta)\n",
    "            v = np.ones(ng)\n",
    "            while all(v == 1):  # mutate until a change occurs (prevent duplicates)\n",
    "                v = (\n",
    "                    g * (npr.random(ng) < mp) * npr.randn(ng) * npr.random() * s + 1\n",
    "                ).clip(0.3, 3.0)\n",
    "\n",
    "            # for i, k in enumerate(hyp.keys()):  # plt.hist(v.ravel(), 300)\n",
    "            keys = [k for k in hyp.keys() if k != \"anchors\"]\n",
    "            # print(len(keys))\n",
    "            for i, k in enumerate(keys):  # plt.hist(v.ravel(), 300)\n",
    "                hyp[k] = float(x[i + 7] * v[i])  # mutate\n",
    "\n",
    "        # Constrain to limits\n",
    "        for k, v in meta.items():\n",
    "            hyp[k] = max(hyp[k], v[1])  # lower limit\n",
    "            hyp[k] = min(hyp[k], v[2])  # upper limit\n",
    "            hyp[k] = round(hyp[k], 5)  # significant digits\n",
    "\n",
    "        # Train mutation\n",
    "        opt_evolve = opt  # .copy()\n",
    "        opt_evolve.epochs = 50  # FIXME: evolution epochs\n",
    "        results = train(hyp.copy(), opt_evolve, device)\n",
    "\n",
    "        # Write mutation results\n",
    "        # print_mutation(hyp.copy(), results, yaml_file, opt.bucket)\n",
    "        hyp_plot = hyp.copy()\n",
    "        del hyp_plot[\"anchors\"]\n",
    "        print_mutation(hyp_plot, results, yaml_file, opt.bucket)\n",
    "\n",
    "    # Plot results\n",
    "    plot_evolution(yaml_file)\n",
    "    print(\n",
    "        f\"Hyperparameter evolution complete. Best results saved as: {yaml_file}\\n\"\n",
    "        f\"Command to train a new model with these hyperparameters: $ python train.py --hyp {yaml_file}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f711364-b7b4-4e68-83b8-37fa91f03ed4",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Visualize results for the trained model.\n",
    "\n",
    "## Plot\n",
    "\n",
    "This cell is made from `plot_results` function from [plots.py](https://github.com/ValV/yolov7/blob/master/yolov7/utils/plots.py) file.\n",
    "\n",
    "This cell displays training and validation metrics in separate plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4436e-54a7-4828-84b3-2e6d2973c85f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from os import path as osp\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "ids = ()\n",
    "labels = ()\n",
    "start, stop = 0, 0\n",
    "\n",
    "fig, ax = plt.subplots(5, 2, figsize=(12, 24), tight_layout=True)\n",
    "ax = ax.ravel()\n",
    "\n",
    "s = [\n",
    "    \"Box\",\n",
    "    \"val Box\",\n",
    "    \"Objectness\",\n",
    "    \"val Objectness\",\n",
    "    \"Classification\",\n",
    "    \"val Classification\",\n",
    "    \"Precision\",\n",
    "    \"Recall\",\n",
    "    \"mAP@0.5\",\n",
    "    \"mAP@0.5:0.95\",\n",
    "]\n",
    "\n",
    "if opt.bucket:\n",
    "    # files = ['https://storage.googleapis.com/%s/results%g.txt' % (bucket, x) for x in id]\n",
    "    files = [\"results%g.txt\" % x for x in ids]\n",
    "    c = (\"gsutil cp \" + \"%s \" * len(files) + \".\") % tuple(\n",
    "        \"gs://%s/results%g.txt\" % (opt.bucket, x) for x in ids\n",
    "    )\n",
    "    os.system(c)\n",
    "else:\n",
    "    files = list(Path(opt.save_dir).glob(\"results*.txt\"))\n",
    "\n",
    "if len(files):\n",
    "    for fi, f in enumerate(files):\n",
    "        try:\n",
    "            results = np.loadtxt(\n",
    "                f, usecols=[2, 3, 4, 8, 9, 12, 13, 14, 10, 11], ndmin=2\n",
    "            ).T\n",
    "            n = results.shape[1]  # number of rows\n",
    "            x = range(start, min(stop, n) if stop else n)\n",
    "            for i in range(10):\n",
    "                y = results[i, x]\n",
    "                if i in [0, 1, 2, 5, 6, 7]:\n",
    "                    y[y == 0] = np.nan  # don't show zero loss values\n",
    "                    # y /= y[0]  # normalize\n",
    "                label = labels[fi] if len(labels) else f.parent.stem\n",
    "                ax[i].plot(x, y, marker=\".\", label=label, linewidth=2, markersize=8)\n",
    "                ax[i].set_title(s[i])\n",
    "                # if i in [5, 6, 7]:  # share train and val loss y axes\n",
    "                #     ax[i].get_shared_y_axes().join(ax[i], ax[i - 5])\n",
    "        except Exception as e:\n",
    "            print(\"Warning: Plotting error for %s; %s\" % (f, e))\n",
    "\n",
    "    ax[1].legend()\n",
    "    # fig.savefig(Path(save_dir) / 'results.png', dpi=200)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\n",
    "        \"No results.txt files found in %s, nothing to plot.\" % osp.abspath(opt.save_dir)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe24220-03f8-4fcc-8312-a614fe1ed12b",
   "metadata": {},
   "source": [
    "## Combine\n",
    "\n",
    "Display combined in a single plot training and validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f34144-a8f0-4897-81dc-0740c87b5c90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = [\n",
    "    \"train\",\n",
    "    \"train\",\n",
    "    \"train\",\n",
    "    \"Precision\",\n",
    "    \"mAP@0.5\",\n",
    "    \"val\",\n",
    "    \"val\",\n",
    "    \"val\",\n",
    "    \"Recall\",\n",
    "    \"mAP@0.5:0.95\",\n",
    "]  # legends\n",
    "t = [\"Box\", \"Objectness\", \"Classification\", \"P-R\", \"mAP-F1\"]  # titles\n",
    "\n",
    "for f in sorted(files):\n",
    "    results = np.loadtxt(f, usecols=[2, 3, 4, 8, 9, 12, 13, 14, 10, 11], ndmin=2).T\n",
    "    n = results.shape[1]  # number of rows\n",
    "    x = range(start, min(stop, n) if stop else n)\n",
    "    fig, ax = plt.subplots(5, 1, figsize=(12, 24), tight_layout=True)\n",
    "    ax = ax.ravel()\n",
    "    for i in range(5):\n",
    "        for j in [i, i + 5]:\n",
    "            y = results[j, x]\n",
    "            ax[i].plot(x, y, marker=\".\", label=s[j])\n",
    "            # y_smooth = butter_lowpass_filtfilt(y)\n",
    "            # ax[i].plot(x, np.gradient(y_smooth), marker='.', label=s[j])\n",
    "\n",
    "        ax[i].set_title(t[i])\n",
    "        ax[i].legend()\n",
    "        # ax[i].set_ylabel(f.parent.stem) if i == 0 else None  # add filename\n",
    "    # fig.savefig(f.replace('.txt', '.png'), dpi=200)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f4d7d3-ef94-4db9-bc12-7627459138b2",
   "metadata": {},
   "source": [
    "# Export\n",
    "\n",
    "The third elephant - `export` function. Export PyTorch model to ONNX format.\n",
    "\n",
    "The `export` function is rendered from [export.py](https://github.com/ValV/yolov7/blob/master/export.py).\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "Extra dependencies (including Nvidia TensorRT):\n",
    "* `nvidia-pyindex` - necessary for correct Graph Surgeon installation;\n",
    "* `onnx-simplifier` - model optimization support during export;\n",
    "* `onnx-graphsurgeon` - Graph Surgeon for adding NMS into ONNX model;\n",
    "* `protobuf` - correct version of protobuf (pain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a4eafc-e610-44dc-99dc-db215bf34128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U 'setuptools' 'nvidia-pyindex'\n",
    "# %pip install 'onnx>=1.9.0'\n",
    "%pip install 'onnx-simplifier>=0.3.6' 'onnx-graphsurgeon' 'protobuf~=3.19.6'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f1ced",
   "metadata": {},
   "source": [
    "## Function\n",
    "\n",
    "NOTE: this function was intended for ONNX export, so export to other formats will require some code modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d24db-0559-4ce0-bada-55caeb1fb837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from yolov7.models.common import Conv\n",
    "from yolov7.models.experimental import attempt_load, End2End\n",
    "from yolov7.utils.activations import Hardswish, SiLU\n",
    "from yolov7.utils.general import set_logging, check_img_size\n",
    "from yolov7.utils.torch_utils import select_device\n",
    "from yolov7.utils.add_nms import RegisterNMS\n",
    "\n",
    "\n",
    "def export(\n",
    "    weights: str,  # weights path\n",
    "    img_size: int = [640, 640],  # image size\n",
    "    batch_size: int = 1,  # batch size\n",
    "    dynamic: bool = False,  # dynamic ONNX axes\n",
    "    dynamic_batch: bool = False,  # dynamic batch onnx for tensorrt and onnx-runtime (disables dynamic axes)\n",
    "    grid: bool = False,  # export Detect() layer grid\n",
    "    end2end: bool = False,  # export end2end onnx (disables dynamic axes)\n",
    "    max_wh: int = None,  # None for tensorrt nms, int value for onnx-runtime nms\n",
    "    topk_all: int = 100,  # topk objects for every image\n",
    "    iou_thres: float = 0.45,  # iou threshold for NMS\n",
    "    conf_thres: float = 0.25,  # conf threshold for NMS\n",
    "    device: str = \"cpu\",  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "    simplify: bool = False,  # simplify onnx model\n",
    "    include_nms: bool = False,  # export end2end onnx\n",
    "    fp16: bool = False,  # CoreML FP16 half-precision export\n",
    "    int8: bool = False,  # CoreML INT8 quantization\n",
    "):\n",
    "    img_size *= 2 if len(img_size) == 1 else 1  # expand\n",
    "    dynamic = dynamic and not end2end\n",
    "    dynamic = False if dynamic_batch else dynamic\n",
    "    set_logging()\n",
    "    t = time.time()\n",
    "\n",
    "    # Load PyTorch model\n",
    "    device = select_device(device)\n",
    "    model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "    labels = model.names\n",
    "\n",
    "    # Checks\n",
    "    gs = int(max(model.stride))  # grid size (max stride)\n",
    "    img_size = [\n",
    "        check_img_size(x, gs) for x in img_size\n",
    "    ]  # verify img_size are gs-multiples\n",
    "\n",
    "    # Input\n",
    "    img = torch.zeros(batch_size, 3, *img_size).to(\n",
    "        device\n",
    "    )  # image size(1, 3, 320, 192) iDetection\n",
    "\n",
    "    # Update model\n",
    "    for k, m in model.named_modules():\n",
    "        m._non_persistent_buffers_set = set()  # pytorch 1.6.0 compatibility\n",
    "        if isinstance(m, Conv):  # assign export-friendly activations\n",
    "            if isinstance(m.act, nn.Hardswish):\n",
    "                m.act = Hardswish()\n",
    "            elif isinstance(m.act, nn.SiLU):\n",
    "                m.act = SiLU()\n",
    "        # elif isinstance(m, models.yolo.Detect):\n",
    "        #     m.forward = m.forward_export  # assign forward (optional)\n",
    "    model.model[-1].export = not grid  # set Detect() layer grid export\n",
    "    y = model(img)  # dry run\n",
    "    if include_nms:\n",
    "        model.model[-1].include_nms = True\n",
    "        y = None\n",
    "\n",
    "    # TorchScript export\n",
    "    try:\n",
    "        print(\"\\nStarting TorchScript export with torch %s...\" % torch.__version__)\n",
    "        f = weights.replace(\".pt\", \".torchscript.pt\")  # filename\n",
    "        ts = torch.jit.trace(model, img, strict=False)\n",
    "        ts.save(f)\n",
    "        print(\"TorchScript export success, saved as %s\" % f)\n",
    "    except Exception as e:\n",
    "        print(\"TorchScript export failure: %s\" % e)\n",
    "\n",
    "    # CoreML export\n",
    "    try:\n",
    "        import coremltools as ct\n",
    "\n",
    "        print(\"\\nStarting CoreML export with coremltools %s...\" % ct.__version__)\n",
    "        # convert model from torchscript and apply pixel scaling as per detect.py\n",
    "        ct_model = ct.convert(\n",
    "            ts,\n",
    "            inputs=[\n",
    "                ct.ImageType(\"image\", shape=img.shape, scale=1 / 255.0, bias=[0, 0, 0])\n",
    "            ],\n",
    "        )\n",
    "        bits, mode = (\n",
    "            (8, \"kmeans_lut\") if int8 else (16, \"linear\") if fp16 else (32, None)\n",
    "        )\n",
    "        if bits < 32:\n",
    "            if sys.platform.lower() == \"darwin\":  # quantization only supported on macOS\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\n",
    "                        \"ignore\", category=DeprecationWarning\n",
    "                    )  # suppress numpy==1.20 float warning\n",
    "                    ct_model = (\n",
    "                        ct.models.neural_network.quantization_utils.quantize_weights(\n",
    "                            ct_model, bits, mode\n",
    "                        )\n",
    "                    )\n",
    "            else:\n",
    "                print(\"quantization only supported on macOS, skipping...\")\n",
    "\n",
    "        f = weights.replace(\".pt\", \".mlmodel\")  # filename\n",
    "        ct_model.save(f)\n",
    "        print(\"CoreML export success, saved as %s\" % f)\n",
    "    except Exception as e:\n",
    "        print(\"CoreML export failure: %s\" % e)\n",
    "\n",
    "    # TorchScript-Lite export\n",
    "    try:\n",
    "        print(\"\\nStarting TorchScript-Lite export with torch %s...\" % torch.__version__)\n",
    "        f = weights.replace(\".pt\", \".torchscript.ptl\")  # filename\n",
    "        tsl = torch.jit.trace(model, img, strict=False)\n",
    "        tsl = optimize_for_mobile(tsl)\n",
    "        tsl._save_for_lite_interpreter(f)\n",
    "        print(\"TorchScript-Lite export success, saved as %s\" % f)\n",
    "    except Exception as e:\n",
    "        print(\"TorchScript-Lite export failure: %s\" % e)\n",
    "\n",
    "    # ONNX export\n",
    "    try:\n",
    "        import onnx\n",
    "\n",
    "        print(\"\\nStarting ONNX export with onnx %s...\" % onnx.__version__)\n",
    "        f = weights.replace(\".pt\", \".onnx\")  # filename\n",
    "        model.eval()\n",
    "        output_names = [\"classes\", \"boxes\"] if y is None else [\"output\"]\n",
    "        dynamic_axes = None\n",
    "        if dynamic:\n",
    "            dynamic_axes = {\n",
    "                \"images\": {\n",
    "                    0: \"batch\",\n",
    "                    2: \"height\",\n",
    "                    3: \"width\",\n",
    "                },  # size(1, 3, 640, 640)\n",
    "                \"output\": {0: \"batch\", 2: \"y\", 3: \"x\"},\n",
    "            }\n",
    "        if dynamic_batch:\n",
    "            batch_size = \"batch\"\n",
    "            dynamic_axes = {\n",
    "                \"images\": {\n",
    "                    0: \"batch\",\n",
    "                },\n",
    "            }\n",
    "            if end2end and max_wh is None:\n",
    "                # TensorRT end2end\n",
    "                output_axes = {\n",
    "                    \"num_dets\": {0: \"batch\"},\n",
    "                    \"det_boxes\": {0: \"batch\"},\n",
    "                    \"det_scores\": {0: \"batch\"},\n",
    "                    \"det_classes\": {0: \"batch\"},\n",
    "                }\n",
    "            else:\n",
    "                # Onnxruntime\n",
    "                output_axes = {\n",
    "                    \"output\": {0: \"batch\"},\n",
    "                }\n",
    "            dynamic_axes.update(output_axes)\n",
    "        if grid:\n",
    "            if end2end:\n",
    "                # End2end Detect() layer grid export\n",
    "                print(\n",
    "                    \"\\nStarting export end2end onnx model for %s...\" % \"TensorRT\"\n",
    "                    if max_wh is None\n",
    "                    else \"onnxruntime\"\n",
    "                )\n",
    "                model = End2End(\n",
    "                    model,\n",
    "                    topk_all,\n",
    "                    iou_thres,\n",
    "                    conf_thres,\n",
    "                    max_wh,\n",
    "                    device,\n",
    "                    len(labels),\n",
    "                )\n",
    "                if end2end and max_wh is None:\n",
    "                    # TensorRT end2end\n",
    "                    output_names = [\n",
    "                        \"num_dets\",\n",
    "                        \"det_boxes\",\n",
    "                        \"det_scores\",\n",
    "                        \"det_classes\",\n",
    "                    ]\n",
    "                    shapes = [\n",
    "                        batch_size,\n",
    "                        1,\n",
    "                        batch_size,\n",
    "                        topk_all,\n",
    "                        4,\n",
    "                        batch_size,\n",
    "                        topk_all,\n",
    "                        batch_size,\n",
    "                        topk_all,\n",
    "                    ]\n",
    "                else:\n",
    "                    # Onnxruntime end2end\n",
    "                    output_names = [\"output\"]\n",
    "            else:\n",
    "                # Basic Detect() layer grid export\n",
    "                model.model[-1].concat = True\n",
    "\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            img,\n",
    "            f,\n",
    "            verbose=False,\n",
    "            opset_version=12,\n",
    "            input_names=[\"images\"],\n",
    "            output_names=output_names,\n",
    "            dynamic_axes=dynamic_axes,\n",
    "        )\n",
    "\n",
    "        # Checks\n",
    "        onnx_model = onnx.load(f)  # load onnx model\n",
    "        onnx.checker.check_model(onnx_model)  # check onnx model\n",
    "\n",
    "        if end2end and max_wh is None:\n",
    "            # TensorRT end2end\n",
    "            for i in onnx_model.graph.output:\n",
    "                for j in i.type.tensor_type.shape.dim:\n",
    "                    j.dim_param = str(shapes.pop(0))\n",
    "\n",
    "        # print(onnx.helper.printable_graph(onnx_model.graph))  # print a human readable model\n",
    "\n",
    "        # # Metadata\n",
    "        # d = {'stride': int(max(model.stride))}\n",
    "        # for k, v in d.items():\n",
    "        #     meta = onnx_model.metadata_props.add()\n",
    "        #     meta.key, meta.value = k, str(v)\n",
    "        # onnx.save(onnx_model, f)\n",
    "\n",
    "        if simplify:\n",
    "            try:\n",
    "                import onnxsim\n",
    "\n",
    "                print(\"\\nStarting to simplify ONNX...\")\n",
    "                onnx_model, check = onnxsim.simplify(onnx_model)\n",
    "                assert check, \"assert check failed\"\n",
    "            except Exception as e:\n",
    "                print(f\"Simplifier failure: {e}\")\n",
    "\n",
    "        # print(onnx.helper.printable_graph(onnx_model.graph))  # print a human readable model\n",
    "        onnx.save(onnx_model, f)\n",
    "        print(\"ONNX export success, saved as %s\" % f)\n",
    "\n",
    "        if include_nms:\n",
    "            print(\"Registering NMS plugin for ONNX...\")\n",
    "            mo = RegisterNMS(f)\n",
    "            mo.register_nms()\n",
    "            mo.save(f)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise\n",
    "        print(\"ONNX export failure: %s\" % e)\n",
    "\n",
    "    # Finish\n",
    "    print(\n",
    "        \"\\nExport complete (%.2fs). Visualize with https://github.com/lutzroeder/netron.\"\n",
    "        % (time.time() - t)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333517d-a738-4209-96e9-505562ef637c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64729d98",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Dummy config. The only parameter it requires is `save_dir` - that is the path where experiment results have been written. By default the path `./runs/train/exp-xxx` is set before training.\n",
    "\n",
    "In order to convert model only without running the whole training procedure, set `opt.save_dir` to a custom path where subdirectory `weights` with PyTorch models.\n",
    "\n",
    "> This cell **must** be commented for automatic export after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# from dataclasses import dataclass\n",
    "# from os import path as osp\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class Config: ...\n",
    "\n",
    "\n",
    "# opt = Config()\n",
    "# opt.save_dir = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40203fb",
   "metadata": {},
   "source": [
    "## Export\n",
    "\n",
    "Some notes on exporting:\n",
    "* `path_export_source` - is the path where PyTorch models reside;\n",
    "* `size_input` - the ONNX model input size (necessary for CPU version of NMS).\n",
    "\n",
    "Parameters explanation:\n",
    "* `weights` - a path to a PyTorch model;\n",
    "* `iou_thresh` - NMS IoU threshold value (merge boxes that overlap more than this value), higher values for higher objects density/overlapping;\n",
    "* `conf_thresh` - NMS object confidence (bboxes below this values will be dropped);\n",
    "* `grid` - export last Detect() layer (not quite sure about this argument, but it works);\n",
    "* `end2end` - export the model with NMS embedded into ONNX graph;\n",
    "* `max_wh` - maximum size of NMS matrix;\n",
    "* `simplify` - optimize (fuse some nodes, etc).\n",
    "\n",
    "> If `max_wh` is `None` (default), then NMS in the ONNX model will be a TensorRT ops and will not run on CPU.\n",
    "\n",
    "Options that should not be enabled when exporting end-to-end ONNX model with NMS for CPU:\n",
    "* `dynamic`;\n",
    "* `dynamic_batch`;\n",
    "* `include_nms`.\n",
    "\n",
    "> TODO: try dynamic batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d2c0fd-2cc3-42b3-91fe-89718aea9f76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_export_source = osp.join(opt.save_dir, \"weights\")\n",
    "# path_export_target = osp.join(opt.save_dir, 'models')\n",
    "\n",
    "# makedirs(path_export_target, exist_ok=True)\n",
    "\n",
    "size_input = [3840, 2176]  # [3840, 2160] (4K) + multiple of 32\n",
    "for path_weights in glob(osp.join(path_export_source, \"????.pt\")):\n",
    "    if \"init.pt\" in path_weights:\n",
    "        continue\n",
    "    print(f\"Exporting {osp.basename(path_weights)}...\")\n",
    "    export(\n",
    "        weights=path_weights,\n",
    "        img_size=opt.img_size if hasattr(opt, \"img_size\") else size_input[::-1],\n",
    "        iou_thres=0.25,  # intersection over union threshold for NMS\n",
    "        conf_thres=0.25,  # confidence threshold for NMS\n",
    "        grid=True,\n",
    "        end2end=True,\n",
    "        max_wh=max(size_input)\n",
    "        if not hasattr(opt, \"img_size\")\n",
    "        else max(tuple(opt.img_size))\n",
    "        if hasattr(opt.img_size, \"__iter__\")\n",
    "        else int(opt.img_size),\n",
    "        simplify=True,\n",
    "    )\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9057d8b2",
   "metadata": {},
   "source": [
    "Check exported ONNX models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe149b07-74ae-45ee-b300-8d14d95d22da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%ls {path_export_source}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec50d447-dc95-43e9-9614-202ecf131413",
   "metadata": {},
   "source": [
    "# Archive\n",
    "\n",
    "Pack training and export results into an archive for downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511803a4-c596-41cd-83bd-93ff2e1a613f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import path as osp\n",
    "from shutil import make_archive\n",
    "\n",
    "\n",
    "make_archive(opt.save_dir, \"zip\", osp.dirname(opt.save_dir), osp.basename(opt.save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93604b4-91f9-49aa-be00-a4a5c1e200dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
