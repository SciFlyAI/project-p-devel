{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04163a7c-056b-4b84-a756-96da8a702460",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "\n",
    "Re-install YOLOv7 package and all the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52256233-317d-430a-b70b-ecae4b8489fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip uninstall -y yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c635e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r https://raw.githubusercontent.com/ValV/yolov7/master/requirements.torch.cpu.txt\n",
    "%pip install -r https://raw.githubusercontent.com/ValV/yolov7/master/requirements.txt\n",
    "%pip install -U git+https://github.com/ValV/yolov7.git@package#egg=yolov7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee7c71",
   "metadata": {},
   "source": [
    "# Export\n",
    "\n",
    "The `export` function is rendered from [export.py](https://github.com/ValV/yolov7/blob/master/export.py).\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "Extra dependencies (including Nvidia TensorRT):\n",
    "* `nvidia-pyindex` - necessary for correct Graph Surgeon installation;\n",
    "* `onnx-simplifier` - model optimization support during export;\n",
    "* `onnx-graphsurgeon` - Graph Surgeon for adding NMS into ONNX model;\n",
    "* `protobuf` - correct version of protobuf (pain).\n",
    "\n",
    "> Comment out Nvidia dependencies to have less troubles working in CPU-only environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a4eafc-e610-44dc-99dc-db215bf34128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install -U 'setuptools' 'nvidia-pyindex'\n",
    "%pip install -U 'setuptools'\n",
    "# %pip install 'onnx>=1.9.0'\n",
    "# %pip install 'onnx-simplifier>=0.3.6' 'onnx-graphsurgeon' 'protobuf~=3.19.6'\n",
    "%pip install 'onnx-simplifier>=0.3.6' 'protobuf~=3.19.6'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da005211",
   "metadata": {},
   "source": [
    "## Function\n",
    "\n",
    "NOTE: this function was intended for ONNX export, so export to other formats will require some code modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d24db-0559-4ce0-bada-55caeb1fb837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from os import makedirs, path as osp\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from yolov7.models.common import Conv\n",
    "from yolov7.models.experimental import attempt_load, End2End\n",
    "from yolov7.utils.activations import Hardswish, SiLU\n",
    "from yolov7.utils.general import set_logging, check_img_size\n",
    "from yolov7.utils.torch_utils import select_device\n",
    "from yolov7.utils.add_nms import RegisterNMS\n",
    "\n",
    "\n",
    "def export(\n",
    "    weights: str,  # weights path\n",
    "    img_size: int = [640, 640],  # image size\n",
    "    batch_size: int = 1,  # batch size\n",
    "    dynamic: bool = False,  # dynamic ONNX axes\n",
    "    dynamic_batch: bool = False,  # dynamic batch onnx for tensorrt and onnx-runtime (disables dynamic axes)\n",
    "    grid: bool = False,  # export Detect() layer grid\n",
    "    end2end: bool = False,  # export end2end onnx (disables dynamic axes)\n",
    "    max_wh: int = None,  # None for tensorrt nms, int value for onnx-runtime nms\n",
    "    topk_all: int = 4000,  # topk objects for every image\n",
    "    iou_thres: float = 0.45,  # iou threshold for NMS\n",
    "    conf_thres: float = 0.25,  # conf threshold for NMS\n",
    "    device: str = \"cpu\",  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "    simplify: bool = False,  # simplify onnx model\n",
    "    include_nms: bool = False,  # export end2end onnx\n",
    "    fp16: bool = False,  # CoreML FP16 half-precision export\n",
    "    int8: bool = False,  # CoreML INT8 quantization\n",
    "):\n",
    "    img_size *= 2 if len(img_size) == 1 else 1  # expand\n",
    "    dynamic = dynamic and not end2end\n",
    "    dynamic = False if dynamic_batch else dynamic\n",
    "    set_logging()\n",
    "    t = time.time()\n",
    "\n",
    "    # Load PyTorch model\n",
    "    device = select_device(device)\n",
    "    model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "    labels = model.names\n",
    "\n",
    "    # Checks\n",
    "    gs = int(max(model.stride))  # grid size (max stride)\n",
    "    img_size = [\n",
    "        check_img_size(x, gs) for x in img_size\n",
    "    ]  # verify img_size are gs-multiples\n",
    "\n",
    "    # Input\n",
    "    img = torch.zeros(batch_size, 3, *img_size).to(\n",
    "        device\n",
    "    )  # image size(1, 3, 320, 192) iDetection\n",
    "\n",
    "    # Update model\n",
    "    for k, m in model.named_modules():\n",
    "        m._non_persistent_buffers_set = set()  # pytorch 1.6.0 compatibility\n",
    "        if isinstance(m, Conv):  # assign export-friendly activations\n",
    "            if isinstance(m.act, nn.Hardswish):\n",
    "                m.act = Hardswish()\n",
    "            elif isinstance(m.act, nn.SiLU):\n",
    "                m.act = SiLU()\n",
    "        # elif isinstance(m, models.yolo.Detect):\n",
    "        #     m.forward = m.forward_export  # assign forward (optional)\n",
    "    model.model[-1].export = not grid  # set Detect() layer grid export\n",
    "    y = model(img)  # dry run\n",
    "    if include_nms:\n",
    "        model.model[-1].include_nms = True\n",
    "        y = None\n",
    "\n",
    "    # TorchScript export\n",
    "    try:\n",
    "        print(\"\\nStarting TorchScript export with torch %s...\" % torch.__version__)\n",
    "        f = weights.replace(\".pt\", \".torchscript.pt\")  # filename\n",
    "        ts = torch.jit.trace(model, img, strict=False)\n",
    "        ts.save(f)\n",
    "        print(\"TorchScript export success, saved as %s\" % f)\n",
    "    except Exception as e:\n",
    "        print(\"TorchScript export failure: %s\" % e)\n",
    "\n",
    "    # CoreML export\n",
    "    try:\n",
    "        import coremltools as ct\n",
    "\n",
    "        print(\"\\nStarting CoreML export with coremltools %s...\" % ct.__version__)\n",
    "        # convert model from torchscript and apply pixel scaling as per detect.py\n",
    "        ct_model = ct.convert(\n",
    "            ts,\n",
    "            inputs=[\n",
    "                ct.ImageType(\"image\", shape=img.shape, scale=1 / 255.0, bias=[0, 0, 0])\n",
    "            ],\n",
    "        )\n",
    "        bits, mode = (\n",
    "            (8, \"kmeans_lut\") if int8 else (16, \"linear\") if fp16 else (32, None)\n",
    "        )\n",
    "        if bits < 32:\n",
    "            if sys.platform.lower() == \"darwin\":  # quantization only supported on macOS\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\n",
    "                        \"ignore\", category=DeprecationWarning\n",
    "                    )  # suppress numpy==1.20 float warning\n",
    "                    ct_model = (\n",
    "                        ct.models.neural_network.quantization_utils.quantize_weights(\n",
    "                            ct_model, bits, mode\n",
    "                        )\n",
    "                    )\n",
    "            else:\n",
    "                print(\"quantization only supported on macOS, skipping...\")\n",
    "\n",
    "        f = weights.replace(\".pt\", \".mlmodel\")  # filename\n",
    "        ct_model.save(f)\n",
    "        print(\"CoreML export success, saved as %s\" % f)\n",
    "    except Exception as e:\n",
    "        print(\"CoreML export failure: %s\" % e)\n",
    "\n",
    "    # TorchScript-Lite export\n",
    "    try:\n",
    "        print(\"\\nStarting TorchScript-Lite export with torch %s...\" % torch.__version__)\n",
    "        f = weights.replace(\".pt\", \".torchscript.ptl\")  # filename\n",
    "        tsl = torch.jit.trace(model, img, strict=False)\n",
    "        tsl = optimize_for_mobile(tsl)\n",
    "        tsl._save_for_lite_interpreter(f)\n",
    "        print(\"TorchScript-Lite export success, saved as %s\" % f)\n",
    "    except Exception as e:\n",
    "        print(\"TorchScript-Lite export failure: %s\" % e)\n",
    "\n",
    "    # ONNX export\n",
    "    try:\n",
    "        import onnx\n",
    "\n",
    "        print(\"\\nStarting ONNX export with onnx %s...\" % onnx.__version__)\n",
    "        f = weights.replace(\".pt\", \".onnx\")  # filename\n",
    "        model.eval()\n",
    "        output_names = [\"classes\", \"boxes\"] if y is None else [\"output\"]\n",
    "        dynamic_axes = None\n",
    "        if dynamic:\n",
    "            dynamic_axes = {\n",
    "                \"images\": {\n",
    "                    0: \"batch\",\n",
    "                    2: \"height\",\n",
    "                    3: \"width\",\n",
    "                },  # size(1, 3, 640, 640)\n",
    "                \"output\": {0: \"batch\", 2: \"y\", 3: \"x\"},\n",
    "            }\n",
    "        if dynamic_batch:\n",
    "            batch_size = \"batch\"\n",
    "            dynamic_axes = {\n",
    "                \"images\": {\n",
    "                    0: \"batch\",\n",
    "                },\n",
    "            }\n",
    "            if end2end and max_wh is None:\n",
    "                # TensorRT end2end\n",
    "                output_axes = {\n",
    "                    \"num_dets\": {0: \"batch\"},\n",
    "                    \"det_boxes\": {0: \"batch\"},\n",
    "                    \"det_scores\": {0: \"batch\"},\n",
    "                    \"det_classes\": {0: \"batch\"},\n",
    "                }\n",
    "            else:\n",
    "                # Onnxruntime\n",
    "                output_axes = {\n",
    "                    \"output\": {0: \"batch\"},\n",
    "                }\n",
    "            dynamic_axes.update(output_axes)\n",
    "        if grid:\n",
    "            if end2end:\n",
    "                # End2end Detect() layer grid export\n",
    "                print(\n",
    "                    \"\\nStarting export end2end onnx model for %s...\" % \"TensorRT\"\n",
    "                    if max_wh is None\n",
    "                    else \"onnxruntime\"\n",
    "                )\n",
    "                model = End2End(\n",
    "                    model,\n",
    "                    topk_all,\n",
    "                    iou_thres,\n",
    "                    conf_thres,\n",
    "                    max_wh,\n",
    "                    device,\n",
    "                    len(labels),\n",
    "                )\n",
    "                if end2end and max_wh is None:\n",
    "                    # TensorRT end2end\n",
    "                    output_names = [\n",
    "                        \"num_dets\",\n",
    "                        \"det_boxes\",\n",
    "                        \"det_scores\",\n",
    "                        \"det_classes\",\n",
    "                    ]\n",
    "                    shapes = [\n",
    "                        batch_size,\n",
    "                        1,\n",
    "                        batch_size,\n",
    "                        topk_all,\n",
    "                        4,\n",
    "                        batch_size,\n",
    "                        topk_all,\n",
    "                        batch_size,\n",
    "                        topk_all,\n",
    "                    ]\n",
    "                else:\n",
    "                    # Onnxruntime end2end\n",
    "                    output_names = [\"output\"]\n",
    "            else:\n",
    "                # Basic Detect() layer grid export\n",
    "                model.model[-1].concat = True\n",
    "\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            img,\n",
    "            f,\n",
    "            verbose=False,\n",
    "            opset_version=12,\n",
    "            input_names=[\"images\"],\n",
    "            output_names=output_names,\n",
    "            dynamic_axes=dynamic_axes,\n",
    "        )\n",
    "\n",
    "        # Checks\n",
    "        onnx_model = onnx.load(f)  # load onnx model\n",
    "        onnx.checker.check_model(onnx_model)  # check onnx model\n",
    "\n",
    "        if end2end and max_wh is None:\n",
    "            # TensorRT end2end\n",
    "            for i in onnx_model.graph.output:\n",
    "                for j in i.type.tensor_type.shape.dim:\n",
    "                    j.dim_param = str(shapes.pop(0))\n",
    "\n",
    "        # print(onnx.helper.printable_graph(onnx_model.graph))  # print a human readable model\n",
    "\n",
    "        # # Metadata\n",
    "        # d = {'stride': int(max(model.stride))}\n",
    "        # for k, v in d.items():\n",
    "        #     meta = onnx_model.metadata_props.add()\n",
    "        #     meta.key, meta.value = k, str(v)\n",
    "        # onnx.save(onnx_model, f)\n",
    "\n",
    "        if simplify:\n",
    "            try:\n",
    "                import onnxsim\n",
    "\n",
    "                print(\"\\nStarting to simplify ONNX...\")\n",
    "                onnx_model, check = onnxsim.simplify(onnx_model)\n",
    "                assert check, \"assert check failed\"\n",
    "            except Exception as e:\n",
    "                print(f\"Simplifier failure: {e}\")\n",
    "\n",
    "        # print(onnx.helper.printable_graph(onnx_model.graph))  # print a human readable model\n",
    "        onnx.save(onnx_model, f)\n",
    "        print(\"ONNX export success, saved as %s\" % f)\n",
    "\n",
    "        if include_nms:\n",
    "            print(\"Registering NMS plugin for ONNX...\")\n",
    "            mo = RegisterNMS(f)\n",
    "            mo.register_nms()\n",
    "            mo.save(f)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise\n",
    "        print(\"ONNX export failure: %s\" % e)\n",
    "\n",
    "    # Finish\n",
    "    print(\n",
    "        \"\\nExport complete (%.2fs). Visualize with https://github.com/lutzroeder/netron.\"\n",
    "        % (time.time() - t)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e124bd",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Dummy config. The only parameter it requires is `save_dir` - that is the path where experiment results have been written. By default the path `./runs/train/exp-xxx` is set before training.\n",
    "\n",
    "In order to convert model only without running the whole training procedure, set `opt.save_dir` to a custom path where subdirectory `weights` with PyTorch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config: ...\n",
    "\n",
    "\n",
    "opt = Config()\n",
    "opt.save_dir = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedefc9f",
   "metadata": {},
   "source": [
    "Custom path constant was intentionally extracted because this notebook is separated from training and its config. Setting this constant to a custom directory takes off conventions and restrictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7585d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_EXPORT_SOURCE = osp.join(opt.save_dir, \"weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd594781",
   "metadata": {},
   "source": [
    "## Export\n",
    "\n",
    "Some notes on exporting:\n",
    "* `path_export_source` - is the path where PyTorch models reside;\n",
    "* `size_input` - the ONNX model input size (necessary for CPU version of NMS).\n",
    "\n",
    "Parameters explanation:\n",
    "* `weights` - a path to a PyTorch model;\n",
    "* `iou_thresh` - NMS IoU threshold value (merge boxes that overlap more than this value), higher values for higher objects density/overlapping;\n",
    "* `conf_thresh` - NMS object confidence (bboxes below this values will be dropped);\n",
    "* `grid` - export last Detect() layer (not quite sure about this argument, but it works);\n",
    "* `end2end` - export the model with NMS embedded into ONNX graph;\n",
    "* `max_wh` - maximum size of NMS matrix;\n",
    "* `simplify` - optimize (fuse some nodes, etc).\n",
    "\n",
    "> If `max_wh` is `None` (default), then NMS in the ONNX model will be a TensorRT ops and will not run on CPU.\n",
    "\n",
    "Options that should not be enabled when exporting end-to-end ONNX model with NMS for CPU:\n",
    "* `dynamic`;\n",
    "* `dynamic_batch`;\n",
    "* `include_nms`.\n",
    "\n",
    "> TODO: try dynamic batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d2c0fd-2cc3-42b3-91fe-89718aea9f76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_export_source = PATH_EXPORT_SOURCE\n",
    "# path_export_target = path_export_source\n",
    "\n",
    "# makedirs(path_export_target, exist_ok=True)\n",
    "\n",
    "size_input = [3840, 2176]  # [3840, 2160] (4K) + multiple of 32\n",
    "for path_weights in glob(osp.join(path_export_source, \"????.pt\")):\n",
    "    if \"init.pt\" in path_weights:\n",
    "        continue\n",
    "    print(f\"Exporting {osp.basename(path_weights)}...\")\n",
    "    export(\n",
    "        weights=path_weights,\n",
    "        img_size=size_input[::-1],  # opt.img_size,\n",
    "        # dynamic=True,\n",
    "        # dynamic_batch=True,\n",
    "        iou_thres=0.25,  # iou threshold for NMS\n",
    "        conf_thres=0.25,  # conf threshold for NMS\n",
    "        grid=True,\n",
    "        end2end=True,\n",
    "        max_wh=max(size_input),\n",
    "        simplify=True,\n",
    "        # include_nms=True,\n",
    "    )\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee2a81",
   "metadata": {},
   "source": [
    "Check exported ONNX models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe149b07-74ae-45ee-b300-8d14d95d22da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%ls {path_export_source}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec50d447-dc95-43e9-9614-202ecf131413",
   "metadata": {},
   "source": [
    "# Archive\n",
    "\n",
    "Pack training and export results into an archive for downloading.\n",
    "\n",
    "> Not necessary for local conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511803a4-c596-41cd-83bd-93ff2e1a613f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from os import path as osp\n",
    "# from shutil import make_archive\n",
    "\n",
    "\n",
    "# make_archive(opt.save_dir, \"zip\", osp.dirname(opt.save_dir), osp.basename(opt.save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93604b4-91f9-49aa-be00-a4a5c1e200dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
